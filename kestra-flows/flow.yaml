id: ai-incident-commander
namespace: hackathon.sre

description: |
  AI-powered autonomous incident response system that monitors, analyzes, and remediates production incidents.
  Fully mock-integrated for demo (GitHub, PagerDuty, Slack, Telemetry) with Artillery load testing.

inputs:
  - id: github_token
    type: STRING
    required: true
    defaults: "abc123"

  - id: pagerduty_token
    type: STRING
    required: true
    defaults: "abc123"

  - id: slack_webhook
    type: STRING
    required: true
    defaults: "http://host.docker.internal:3001/slack"

  - id: deployments_repo
    type: STRING
    defaults: "demo/service"

  - id: k8s_namespace
    type: STRING
    defaults: "production"

  - id: service_name
    type: STRING
    defaults: "payment-api"

  - id: service_url
    type: STRING
    defaults: "http://host.docker.internal:3000"

  - id: environment
    type: STRING
    defaults: "production"

  - id: metrics_url
    type: STRING
    defaults: "http://host.docker.internal:3002/metrics"

  - id: logs_url
    type: STRING
    defaults: "http://host.docker.internal:3002/logs"

  - id: alerts_url
    type: STRING
    defaults: "http://host.docker.internal:3002/alerts"

  - id: min_confidence_for_rollback
    type: INT
    defaults: 80

  - id: enable_auto_remediation
    type: BOOLEAN
    defaults: true

  - id: load_test_duration
    type: INT
    defaults: 30
    description: "Duration of load test in seconds"

  - id: load_test_rate
    type: INT
    defaults: 10
    description: "Requests per second for load test"

tasks:

  # ============================================
  # PHASE 1 ‚Äî OBSERVE
  # ============================================
  - id: collect_signals
    type: io.kestra.plugin.core.flow.Parallel
    tasks:

      - id: fetch_metrics
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.metrics_url }}"
        method: GET

      - id: fetch_logs
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.logs_url }}"
        method: GET

      - id: fetch_alerts
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.alerts_url }}"
        method: GET

      - id: fetch_deployments
        type: io.kestra.plugin.core.http.Request
        uri: "http://host.docker.internal:5000/repos/demo/service/deployments"
        method: GET
        headers:
          Authorization: "Bearer {{ inputs.github_token }}"
        contentType: application/json

      - id: run_load_test
        type: io.kestra.plugin.docker.Run
        containerImage: swymbnsl/artillery-runner:slim
        inputFiles:
          load-test.yml: |
            config:
              target: "{{ inputs.service_url }}"
              phases:
                - duration: {{ inputs.load_test_duration }}
                  arrivalRate: {{ inputs.load_test_rate }}
              processor: "./processor.js"
            scenarios:
              - name: "Health Check"
                flow:
                  - get:
                      url: "/health"
              - name: "Payment API"
                weight: 70
                flow:
                  - post:
                      url: "/api/payment"
                      json:
                        amount: 100
                        currency: "USD"
              - name: "User Profile"
                weight: 30
                flow:
                  - get:
                      url: "/api/user/profile"
          processor.js: |
            module.exports = {
              beforeScenario: (userContext, events, done) => {
                userContext.vars.timestamp = Date.now();
                return done();
              }
            };
        commands:
          - artillery
          - run
          - load-test.yml
          - -o
          - results.json
        outputFiles:
          - results.json

  # ============================================
  # PHASE 1.5 ‚Äî PARSE LOAD TEST RESULTS
  # ============================================
  - id: parse_load_test_results
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    beforeCommands:
      - pip install kestra
    script: |
      import json
      import sys
      from kestra import Kestra
      
      try:
          raw = """{{ outputs.run_load_test.outputFiles['results.json'] }}"""
          data = json.loads(raw)
          
          # Extract aggregate stats
          aggregate = data.get('aggregate', {})
          
          # Get latency percentiles
          latencies = aggregate.get('latencies', {})
          p95 = latencies.get('p95', 0) / 1000  # Convert to ms
          p99 = latencies.get('p99', 0) / 1000
          median = latencies.get('median', 0) / 1000
          
          # Get request stats
          counters = aggregate.get('counters', {})
          total_requests = counters.get('http.requests', 0)
          
          # Get status codes
          codes = aggregate.get('codes', {})
          success_codes = codes.get('200', 0) + codes.get('201', 0) + codes.get('204', 0)
          error_codes = sum(codes.get(str(code), 0) for code in range(400, 600))
          
          # Calculate error rate
          error_rate = (error_codes / total_requests * 100) if total_requests > 0 else 0
          
          # Get RPS
          rates = aggregate.get('rates', {})
          rps = rates.get('http.request_rate', 0)
          
          # Output structured data
          Kestra.outputs({
              'p95_latency': round(p95, 2),
              'p99_latency': round(p99, 2),
              'median_latency': round(median, 2),
              'rps': round(rps, 2),
              'total_requests': total_requests,
              'successful_requests': success_codes,
              'failed_requests': error_codes,
              'error_rate': round(error_rate, 2)
          })
          
          print(f"Load test completed:", file=sys.stderr)
          print(f"  Total requests: {total_requests}", file=sys.stderr)
          print(f"  Error rate: {error_rate:.2f}%", file=sys.stderr)
          print(f"  P95 latency: {p95:.2f}ms", file=sys.stderr)
          print(f"  P99 latency: {p99:.2f}ms", file=sys.stderr)
          
      except Exception as e:
          print(f"Error parsing load test results: {e}", file=sys.stderr)
          # Output default values on error
          Kestra.outputs({
              'p95_latency': 0,
              'p99_latency': 0,
              'median_latency': 0,
              'rps': 0,
              'total_requests': 0,
              'successful_requests': 0,
              'failed_requests': 0,
              'error_rate': 0
          })

  # ============================================
  # PHASE 2 ‚Äî ANALYZE
  # ============================================
  - id: ai_incident_analysis
    type: io.kestra.plugin.openai.ChatCompletion
    description: AI decision engine
    apiKey: "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"
    model: "gpt-4.1-mini"
    temperature: 0.3
    messages:
      - role: system
        content: |
          You MUST respond with exactly this JSON structure:

          {
            "severity": "P0" | "P1" | "P2" | "P3",
            "confidence_score": 85,
            "incident_summary": "string",
            "root_cause_hypothesis": "string",
            "deployment_to_rollback": 101,
            "recommended_action": "rollback" | "monitor" | "escalate"
          }

          Rules:
          - confidence_score: 0-100, how confident you are in your analysis
          - Use "rollback" only if you're confident the deployment caused the issue
          - Use "monitor" for unclear situations
          - Use "escalate" for complex issues needing human intervention
          - Consider load test results heavily - high error rates and latency are strong signals
          - P95 latency > 500ms or error rate > 5% are critical indicators
          
          NO OTHER FIELDS.
          NO nested objects.
          NO markdown.
          ONLY this JSON object.

      - role: user
        content: |
          SERVICE: {{ inputs.service_name }}
          ENVIRONMENT: {{ inputs.environment }}
          TIMESTAMP: {{ execution.startDate }}

          METRICS:
          {{ outputs.fetch_metrics.body }}

          LOGS:
          {{ outputs.fetch_logs.body }}

          ALERTS:
          {{ outputs.fetch_alerts.body }}

          DEPLOYMENTS:
          {{ outputs.fetch_deployments.body }}

          LOAD TEST RESULTS:
          - Total Requests: {{ outputs.parse_load_test_results.vars.total_requests }}
          - Successful: {{ outputs.parse_load_test_results.vars.successful_requests }}
          - Failed: {{ outputs.parse_load_test_results.vars.failed_requests }}
          - Error Rate: {{ outputs.parse_load_test_results.vars.error_rate }}%
          - P95 Latency: {{ outputs.parse_load_test_results.vars.p95_latency }}ms
          - P99 Latency: {{ outputs.parse_load_test_results.vars.p99_latency }}ms
          - Median Latency: {{ outputs.parse_load_test_results.vars.median_latency }}ms
          - Requests/sec: {{ outputs.parse_load_test_results.vars.rps }}

          MIN_CONFIDENCE_THRESHOLD: {{ inputs.min_confidence_for_rollback }}

          Analyze the incident and provide your assessment with confidence score.
          Pay special attention to the load test results - they provide real-time performance data.
          Respond with ONLY valid JSON.

  # ============================================
  # PHASE 3 ‚Äî PARSE AI OUTPUT
  # ============================================
  - id: parse_ai_decision
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    beforeCommands:
      - pip install kestra
    script: |
      import json
      import sys
      from kestra import Kestra

      raw_content = """{{ outputs.ai_incident_analysis.choices[0].message.content }}"""
      
      content = raw_content.strip()
      
      # Remove markdown code blocks if present
      if content.startswith('```'):
          content = content[content.find('\n')+1:]
      if content.endswith('```'):
          content = content[:content.rfind('\n')]
      
      content = content.strip()
      
      try:
          data = json.loads(content)
      except json.JSONDecodeError as e:
          print(f"ERROR: Failed to parse JSON: {e}", file=sys.stderr)
          print(f"Raw content: {content}", file=sys.stderr)
          sys.exit(1)
      
      # Output variables using Kestra.outputs()
      Kestra.outputs({
          'severity': data.get('severity', 'UNKNOWN'),
          'confidence_score': data.get('confidence_score', 0),
          'incident_summary': data.get('incident_summary', 'No summary'),
          'root_cause_hypothesis': data.get('root_cause_hypothesis', 'Unknown'),
          'deployment_to_rollback': data.get('deployment_to_rollback', 0),
          'recommended_action': data.get('recommended_action', 'monitor')
      })
      
      # Debug output
      print(json.dumps(data, indent=2), file=sys.stderr)

  # ============================================
  # PHASE 4 ‚Äî ROUTE BY SEVERITY
  # ============================================
  - id: route_by_severity
    type: io.kestra.plugin.core.flow.Switch
    value: "{{ outputs.parse_ai_decision.vars.severity }}"
    cases:

      P0:
        - id: handle_p0
          type: io.kestra.plugin.core.flow.Sequential
          tasks:

            # Check confidence before rollback
            - id: evaluate_rollback_decision
              type: io.kestra.plugin.core.flow.If
              condition: "{{ outputs.parse_ai_decision.vars.confidence_score >= inputs.min_confidence_for_rollback and outputs.parse_ai_decision.vars.recommended_action == 'rollback' }}"
              then:
                - id: perform_rollback
                  type: io.kestra.plugin.core.flow.Sequential
                  tasks:
                    - id: rollback
                      type: io.kestra.plugin.core.http.Request
                      uri: "http://host.docker.internal:5000/repos/demo/service/deployments/{{ outputs.parse_ai_decision.vars.deployment_to_rollback }}/statuses"
                      method: POST
                      headers:
                        Authorization: "Bearer {{ inputs.github_token }}"
                      contentType: application/json
                      body: |
                        {
                          "state": "inactive",
                          "description": "Automatic rollback performed by AI Commander (confidence: {{ outputs.parse_ai_decision.vars.confidence_score }}%)"
                        }

                    - id: log_rollback_success
                      type: io.kestra.plugin.core.log.Log
                      message: "Rollback completed for deployment {{ outputs.parse_ai_decision.vars.deployment_to_rollback }}"

              else:
                - id: skip_rollback_log
                  type: io.kestra.plugin.core.log.Log
                  message: "Rollback skipped - Confidence {{ outputs.parse_ai_decision.vars.confidence_score }}% below threshold {{ inputs.min_confidence_for_rollback }}% or action is '{{ outputs.parse_ai_decision.vars.recommended_action }}'"

            - id: pagerduty_mock
              type: io.kestra.plugin.core.http.Request
              uri: "http://host.docker.internal:4000/incidents"
              method: POST
              contentType: application/json
              body: |
                {
                  "incident": {
                    "severity": "P0",
                    "summary": "{{ outputs.parse_ai_decision.vars.incident_summary }}",
                    "root_cause": "{{ outputs.parse_ai_decision.vars.root_cause_hypothesis }}",
                    "confidence": {{ outputs.parse_ai_decision.vars.confidence_score }},
                    "action_taken": "{{ outputs.parse_ai_decision.vars.recommended_action }}",
                    "load_test_error_rate": {{ outputs.parse_load_test_results.vars.error_rate }},
                    "load_test_p99_latency": {{ outputs.parse_load_test_results.vars.p99_latency }}
                  }
                }

            - id: slack_notify_p0
              type: io.kestra.plugin.core.http.Request
              uri: "{{ inputs.slack_webhook }}"
              method: POST
              contentType: application/json
              body: |
                {
                  "blocks": [
                    {
                      "type": "header",
                      "text": {
                        "type": "plain_text",
                        "text": "üö® P0 CRITICAL INCIDENT",
                        "emoji": true
                      }
                    },
                    {
                      "type": "section",
                      "fields": [
                        {
                          "type": "mrkdwn",
                          "text": "*Service:*\n{{ inputs.service_name }}"
                        },
                        {
                          "type": "mrkdwn",
                          "text": "*Environment:*\n{{ inputs.environment }}"
                        },
                        {
                          "type": "mrkdwn",
                          "text": "*Severity:*\nP0"
                        },
                        {
                          "type": "mrkdwn",
                          "text": "*Confidence:*\n{{ outputs.parse_ai_decision.vars.confidence_score }}%"
                        }
                      ]
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Incident Summary:*\n{{ outputs.parse_ai_decision.vars.incident_summary }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Root Cause Hypothesis:*\n{{ outputs.parse_ai_decision.vars.root_cause_hypothesis }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Load Test Results:*\n‚Ä¢ Error Rate: {{ outputs.parse_load_test_results.vars.error_rate }}%\n‚Ä¢ P95 Latency: {{ outputs.parse_load_test_results.vars.p95_latency }}ms\n‚Ä¢ P99 Latency: {{ outputs.parse_load_test_results.vars.p99_latency }}ms\n‚Ä¢ Total Requests: {{ outputs.parse_load_test_results.vars.total_requests }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Action Taken:*\n{{ outputs.parse_ai_decision.vars.recommended_action }}"
                      }
                    },
                    {
                      "type": "divider"
                    },
                    {
                      "type": "context",
                      "elements": [
                        {
                          "type": "mrkdwn",
                          "text": "Execution ID: {{ execution.id }} | Started: {{ execution.startDate }}"
                        }
                      ]
                    }
                  ]
                }

      P1:
        - id: handle_p1
          type: io.kestra.plugin.core.http.Request
          uri: "{{ inputs.slack_webhook }}"
          method: POST
          contentType: application/json
          body: |
            {
              "blocks": [
                {
                  "type": "header",
                  "text": {
                    "type": "plain_text",
                    "text": "‚ö†Ô∏è P1 MAJOR INCIDENT",
                    "emoji": true
                  }
                },
                {
                  "type": "section",
                  "fields": [
                    {
                      "type": "mrkdwn",
                      "text": "*Service:*\n{{ inputs.service_name }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Confidence:*\n{{ outputs.parse_ai_decision.vars.confidence_score }}%"
                    }
                  ]
                },
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Summary:*\n{{ outputs.parse_ai_decision.vars.incident_summary }}"
                  }
                },
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Load Test:*\nError Rate: {{ outputs.parse_load_test_results.vars.error_rate }}% | P99: {{ outputs.parse_load_test_results.vars.p99_latency }}ms"
                  }
                }
              ]
            }

      P2:
        - id: handle_p2
          type: io.kestra.plugin.core.log.Log
          message: "P2 incident logged: {{ outputs.parse_ai_decision.vars.incident_summary }}"

      P3:
        - id: handle_p3
          type: io.kestra.plugin.core.log.Log
          message: "P3 minor issue logged: {{ outputs.parse_ai_decision.vars.incident_summary }}"

      default:
        - id: handle_invalid_ai_output
          type: io.kestra.plugin.core.log.Log
          message: "Invalid AI output or missing severity."

  # ============================================
  # PHASE 5 ‚Äî HEALTH CHECK AFTER REMEDIATION
  # ============================================
  - id: post_remediation_health_check
    type: io.kestra.plugin.core.flow.Sequential
    tasks:
      
      - id: wait_for_stabilization
        type: io.kestra.plugin.core.flow.Sleep
        duration: PT30S

      - id: run_post_load_test
        type: io.kestra.plugin.docker.Run
        containerImage: swymbnsl/artillery-runner:slim

        inputFiles:
          load-test-post.yml: |
            config:
              target: "{{ inputs.service_url }}"
              phases:
                - duration: {{ inputs.load_test_duration }}
                  arrivalRate: {{ inputs.load_test_rate }}
            scenarios:
              - name: "Health Check"
                flow:
                  - get:
                      url: "/health"
              - name: "Payment API"
                weight: 70
                flow:
                  - post:
                      url: "/api/payment"
                      json:
                        amount: 100
                        currency: "USD"
              - name: "User Profile"
                weight: 30
                flow:
                  - get:
                      url: "/api/user/profile"

        commands:
          - artillery
          - run
          - load-test-post.yml
          - -o
          - results-post.json

        outputFiles:
          - results-post.json


      - id: parse_post_load_test
        type: io.kestra.plugin.scripts.python.Script
        docker:
          image: python:3.11-slim
        beforeCommands:
          - pip install kestra
        script: |
          import json
          import sys
          from kestra import Kestra
          
          try:
              raw = """{{ outputs.run_post_load_test.outputFiles['results-post.json'] }}"""
              data = json.loads(raw)
              
              aggregate = data.get('aggregate', {})
              latencies = aggregate.get('latencies', {})
              p95 = latencies.get('p95', 0) / 1000
              p99 = latencies.get('p99', 0) / 1000
              
              counters = aggregate.get('counters', {})
              total_requests = counters.get('http.requests', 0)
              
              codes = aggregate.get('codes', {})
              success_codes = codes.get('200', 0) + codes.get('201', 0) + codes.get('204', 0)
              error_codes = sum(codes.get(str(code), 0) for code in range(400, 600))
              error_rate = (error_codes / total_requests * 100) if total_requests > 0 else 0
              
              Kestra.outputs({
                  'p95_latency': round(p95, 2),
                  'p99_latency': round(p99, 2),
                  'error_rate': round(error_rate, 2)
              })
              
          except Exception as e:
              print(f"Error parsing post-load test: {e}", file=sys.stderr)
              Kestra.outputs({
                  'p95_latency': 0,
                  'p99_latency': 0,
                  'error_rate': 0
              })

      - id: fetch_post_metrics
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.metrics_url }}"
        method: GET

      - id: fetch_post_alerts
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.alerts_url }}"
        method: GET

      - id: compare_health_status
        type: io.kestra.plugin.scripts.python.Script
        docker:
          image: python:3.11-slim
        beforeCommands:
          - pip install kestra
        script: |
          import json
          import sys
          from kestra import Kestra

          # Get before and after load test results
          before_error_rate = {{ outputs.parse_load_test_results.vars.error_rate }}
          after_error_rate = {{ outputs.parse_post_load_test.vars.error_rate }}
          
          before_p99 = {{ outputs.parse_load_test_results.vars.p99_latency }}
          after_p99 = {{ outputs.parse_post_load_test.vars.p99_latency }}
          
          # Calculate improvements
          if before_error_rate > 0:
              error_improvement_pct = ((before_error_rate - after_error_rate) / before_error_rate) * 100
          else:
              error_improvement_pct = 0
          
          if before_p99 > 0:
              latency_improvement_pct = ((before_p99 - after_p99) / before_p99) * 100
          else:
              latency_improvement_pct = 0
          
          # Determine health status
          if after_error_rate < 1 and latency_improvement_pct > 30:
              health_status = "FULLY_RECOVERED"
          elif after_error_rate < before_error_rate * 0.5:
              health_status = "SIGNIFICANT_IMPROVEMENT"
          elif after_error_rate < before_error_rate:
              health_status = "IMPROVING"
          elif after_error_rate == before_error_rate:
              health_status = "STABLE"
          else:
              health_status = "DEGRADED"
          
          Kestra.outputs({
              'health_status': health_status,
              'before_error_rate': before_error_rate,
              'after_error_rate': after_error_rate,
              'error_improvement_pct': round(error_improvement_pct, 2),
              'before_p99_latency': before_p99,
              'after_p99_latency': after_p99,
              'latency_improvement_pct': round(latency_improvement_pct, 2)
          })
          
          print(f"Health check complete: {health_status}", file=sys.stderr)
          print(f"Error rate: {before_error_rate}% -> {after_error_rate}% ({error_improvement_pct:.2f}% improvement)", file=sys.stderr)
          print(f"P99 latency: {before_p99}ms -> {after_p99}ms ({latency_improvement_pct:.2f}% improvement)", file=sys.stderr)

      - id: notify_health_status
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.slack_webhook }}"
        method: POST
        contentType: application/json
        body: |
          {
            "blocks": [
              {
                "type": "header",
                "text": {
                  "type": "plain_text",
                  "text": "üè• Post-Remediation Health Check",
                  "emoji": true
                }
              },
              {
                "type": "section",
                "fields": [
                  {
                    "type": "mrkdwn",
                    "text": "*Status:*\n{{ outputs.compare_health_status.vars.health_status }}"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*Error Rate:*\n{{ outputs.compare_health_status.vars.before_error_rate }}% ‚Üí {{ outputs.compare_health_status.vars.after_error_rate }}%"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*P99 Latency:*\n{{ outputs.compare_health_status.vars.before_p99_latency }}ms ‚Üí {{ outputs.compare_health_status.vars.after_p99_latency }}ms"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*Improvement:*\nError: {{ outputs.compare_health_status.vars.error_improvement_pct }}%\nLatency: {{ outputs.compare_health_status.vars.latency_improvement_pct }}%"
                  }
                ]
              }
            ]
          }

  # ============================================
  # PHASE 6 ‚Äî FINAL LOG
  # ============================================
  - id: log_incident_outcome
    type: io.kestra.plugin.core.log.Log
    message: |
      ==================== INCIDENT COMPLETE ====================
      Severity: {{ outputs.parse_ai_decision.vars.severity }}
      Confidence: {{ outputs.parse_ai_decision.vars.confidence_score }}%
      Action: {{ outputs.parse_ai_decision.vars.recommended_action }}
      Health Status: {{ outputs.compare_health_status.vars.health_status }}
      Summary: {{ outputs.parse_ai_decision.vars.incident_summary }}
      
      LOAD TEST RESULTS:
      Before - Error Rate: {{ outputs.parse_load_test_results.vars.error_rate }}%, P99: {{ outputs.parse_load_test_results.vars.p99_latency }}ms
      After  - Error Rate: {{ outputs.compare_health_status.vars.after_error_rate }}%, P99: {{ outputs.compare_health_status.vars.after_p99_latency }}ms
      Improvement - Error: {{ outputs.compare_health_status.vars.error_improvement_pct }}%, Latency: {{ outputs.compare_health_status.vars.latency_improvement_pct }}%
      ===========================================================

triggers:
  - id: webhook_alert_trigger
    type: io.kestra.plugin.core.trigger.Webhook
    key: "incident-alert-{{ inputs.service_name }}"