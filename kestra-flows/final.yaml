id: ai-incident-commander
namespace: hackathon.sre

description: |
  AI-powered autonomous incident response system that monitors, analyzes, and remediates production incidents.
  Features: Anomaly detection, chaos engineering mode, autonomous rollback, health monitoring, incident history.
  Integrations: GitHub (deployment tracking), PagerDuty (incident management), Slack (notifications).

inputs:
  - id: github_token
    type: STRING
    required: true
    defaults: "mock"
    description: "GitHub token for deployment history. Use 'mock' for demo mode."

  - id: pagerduty_token
    type: STRING
    required: true
    defaults: "mock"
    description: "PagerDuty API token. Use 'mock' for demo mode."

  - id: slack_webhook
    type: STRING
    required: true
    defaults: "https://hooks.slack.com/services/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"
    description: "Enter your own slack webhook here to recieve messages."

  - id: deployments_repo
    type: STRING
    defaults: "demo/service"
    description: "GitHub repo in format 'owner/repo'"

  - id: service_name
    type: STRING
    defaults: "payment-api"

  - id: service_url
    type: STRING
    defaults: "http://payment-api:3000"

  - id: environment
    type: STRING
    defaults: "production"

  - id: metrics_url
    type: STRING
    defaults: "http://payment-api:3000/metrics/json"

  - id: logs_url
    type: STRING
    defaults: "http://payment-api:3000/logs"

  - id: alerts_url
    type: STRING
    defaults: "http://payment-api:3000/alerts"

  - id: min_confidence_for_rollback
    type: INT
    defaults: 80

  - id: chaos_mode
    type: BOOLEAN
    defaults: false
    description: "Enable chaos injection for testing (injects 600ms latency)"

  - id: pagerduty_service_id
    type: STRING
    defaults: "PSERVICE1"
    description: "PagerDuty service ID for incident creation"

  - id: pagerduty_escalation_policy
    type: STRING
    defaults: "PEPOLICY1"
    description: "PagerDuty escalation policy ID"

  - id: chaos_type
    type: STRING
    defaults: "latency"
    description: "Type of chaos: latency, cpu-stress, memory-stress, network-delay, pod-kill"

  - id: chaos_duration
    type: STRING
    defaults: "30s"
    description: "Duration for chaos experiment (e.g., 30s, 1m, 2m)"

  - id: chaos_intensity
    type: STRING
    defaults: "medium"
    description: "Intensity: low, medium, high"

  - id: kubernetes_api_url
    type: STRING
    defaults: "https://kubernetes.default.svc"
    description: "Kubernetes API URL (if using Chaos Mesh)"

  - id: kubernetes_token
    type: STRING
    defaults: "mock"
    description: "Kubernetes service account token (if using Chaos Mesh)"

  - id: chaos_namespace
    type: STRING
    defaults: "default"
    description: "Kubernetes namespace for chaos experiments"

  - id: chaos_label_selector
    type: STRING
    defaults: "app=payment-api"
    description: "Label selector for targeting pods"

tasks:

  # PHASE 0 ‚Äî CHAOS INJECTION (OPTIONAL)
  - id: chaos_inject
    type: io.kestra.plugin.core.flow.If
    condition: "{{ inputs.chaos_mode }}"
    then:
      - id: determine_chaos_config
        type: io.kestra.plugin.scripts.python.Script
        docker:
          image: python:3.11-slim
        beforeCommands:
          - pip install kestra
        script: |
          import json
          import sys
          from kestra import Kestra
          
          chaos_type = "{{ inputs.chaos_type }}"
          intensity = "{{ inputs.chaos_intensity }}"
          duration = "{{ inputs.chaos_duration }}"
          
          # Define chaos configurations based on type and intensity
          configs = {
              "latency": {
                  "low": {"latency_ms": 200, "method": "http"},
                  "medium": {"latency_ms": 600, "method": "http"},
                  "high": {"latency_ms": 1500, "method": "http"}
              },
              "cpu-stress": {
                  "low": {"workers": 1, "load": 40, "method": "chaos-mesh"},
                  "medium": {"workers": 1, "load": 80, "method": "chaos-mesh"},
                  "high": {"workers": 2, "load": 95, "method": "chaos-mesh"}
              },
              "memory-stress": {
                  "low": {"workers": 1, "size": "64MB", "method": "chaos-mesh"},
                  "medium": {"workers": 1, "size": "256MB", "method": "chaos-mesh"},
                  "high": {"workers": 2, "size": "512MB", "method": "chaos-mesh"}
              },
              "network-delay": {
                  "low": {"latency": "100ms", "jitter": "10ms", "method": "chaos-mesh"},
                  "medium": {"latency": "500ms", "jitter": "50ms", "method": "chaos-mesh"},
                  "high": {"latency": "2000ms", "jitter": "200ms", "method": "chaos-mesh"}
              },
              "pod-kill": {
                  "low": {"method": "chaos-mesh", "grace_period": "10s"},
                  "medium": {"method": "chaos-mesh", "grace_period": "5s"},
                  "high": {"method": "chaos-mesh", "grace_period": "0s"}
              }
          }
          
          config = configs.get(chaos_type, {}).get(intensity, configs["latency"]["medium"])
          config["chaos_type"] = chaos_type
          config["duration"] = duration
          
          Kestra.outputs({
              "config": json.dumps(config),
              "method": config.get("method", "http"),
              "chaos_type": chaos_type,
              "latency_ms": config.get("latency_ms", 600),
              "workers": config.get("workers", 1),
              "load": config.get("load", 80),
              "size": config.get("size", "128MB"),
              "latency": config.get("latency", "500ms"),
              "jitter": config.get("jitter", "50ms"),
              "grace_period": config.get("grace_period", "5s")
          })
          
          print(f"üå™Ô∏è Chaos Config: {chaos_type} ({intensity})", file=sys.stderr)
          print(f"   Duration: {duration}", file=sys.stderr)
          print(f"   Method: {config.get('method')}", file=sys.stderr)

      - id: route_chaos_method
        type: io.kestra.plugin.core.flow.Switch
        value: "{{ outputs.determine_chaos_config.vars.method }}"
        cases:
          
          # HTTP-based chaos (simple latency injection)
          http:
            - id: inject_http_latency
              type: io.kestra.plugin.core.http.Request
              uri: "{{ inputs.service_url }}/chaos/latency"
              method: POST
              contentType: application/json
              body: |
                {
                  "latency_ms": {{ outputs.determine_chaos_config.vars.latency_ms }},
                  "duration_seconds": 30,
                  "reason": "Chaos testing from execution {{ execution.id }}"
                }
              allowFailed: true

            - id: log_http_chaos
              type: io.kestra.plugin.core.log.Log
              message: "üå™Ô∏è HTTP Chaos: Injected {{ outputs.determine_chaos_config.vars.latency_ms }}ms latency for {{ inputs.chaos_duration }}"

          # Chaos Mesh-based chaos experiments
          chaos-mesh:
            - id: check_k8s_token
              type: io.kestra.plugin.core.flow.If
              condition: "{{ inputs.kubernetes_token != 'mock' }}"
              then:
                - id: build_chaos_mesh_manifest
                  type: io.kestra.plugin.scripts.python.Script
                  docker:
                    image: python:3.11-slim
                  beforeCommands:
                    - pip install kestra pyyaml
                  script: |
                    import json
                    import yaml
                    import sys
                    from kestra import Kestra
                    
                    chaos_type = "{{ outputs.determine_chaos_config.vars.chaos_type }}"
                    namespace = "{{ inputs.chaos_namespace }}"
                    label_selector = "{{ inputs.chaos_label_selector }}"
                    duration = "{{ inputs.chaos_duration }}"
                    execution_id = "{{ execution.id }}".replace("_", "-")
                    
                    # Parse label selector
                    label_key, label_value = label_selector.split("=")
                    
                    manifest = None
                    
                    if chaos_type == "cpu-stress":
                        workers = int("{{ outputs.determine_chaos_config.vars.workers }}")
                        load = int("{{ outputs.determine_chaos_config.vars.load }}")
                        
                        manifest = {
                            "apiVersion": "chaos-mesh.org/v1alpha1",
                            "kind": "StressChaos",
                            "metadata": {
                                "name": f"cpu-stress-{execution_id}",
                                "namespace": namespace
                            },
                            "spec": {
                                "mode": "one",
                                "selector": {
                                    "namespaces": [namespace],
                                    "labelSelectors": {label_key: label_value}
                                },
                                "duration": duration,
                                "stressors": {
                                    "cpu": {
                                        "workers": workers,
                                        "load": load
                                    }
                                }
                            }
                        }
                    
                    elif chaos_type == "memory-stress":
                        workers = int("{{ outputs.determine_chaos_config.vars.workers }}")
                        size = "{{ outputs.determine_chaos_config.vars.size }}"
                        
                        manifest = {
                            "apiVersion": "chaos-mesh.org/v1alpha1",
                            "kind": "StressChaos",
                            "metadata": {
                                "name": f"memory-stress-{execution_id}",
                                "namespace": namespace
                            },
                            "spec": {
                                "mode": "one",
                                "selector": {
                                    "namespaces": [namespace],
                                    "labelSelectors": {label_key: label_value}
                                },
                                "duration": duration,
                                "stressors": {
                                    "memory": {
                                        "workers": workers,
                                        "size": size
                                    }
                                }
                            }
                        }
                    
                    elif chaos_type == "network-delay":
                        latency = "{{ outputs.determine_chaos_config.vars.latency }}"
                        jitter = "{{ outputs.determine_chaos_config.vars.jitter }}"
                        
                        manifest = {
                            "apiVersion": "chaos-mesh.org/v1alpha1",
                            "kind": "NetworkChaos",
                            "metadata": {
                                "name": f"network-delay-{execution_id}",
                                "namespace": namespace
                            },
                            "spec": {
                                "action": "delay",
                                "mode": "all",
                                "selector": {
                                    "namespaces": [namespace],
                                    "labelSelectors": {label_key: label_value}
                                },
                                "delay": {
                                    "latency": latency,
                                    "correlation": "100",
                                    "jitter": jitter
                                },
                                "duration": duration,
                                "direction": "to"
                            }
                        }
                    
                    elif chaos_type == "pod-kill":
                        grace_period = "{{ outputs.determine_chaos_config.vars.grace_period }}"
                        
                        manifest = {
                            "apiVersion": "chaos-mesh.org/v1alpha1",
                            "kind": "PodChaos",
                            "metadata": {
                                "name": f"pod-kill-{execution_id}",
                                "namespace": namespace
                            },
                            "spec": {
                                "action": "pod-kill",
                                "mode": "one",
                                "selector": {
                                    "namespaces": [namespace],
                                    "labelSelectors": {label_key: label_value}
                                },
                                "gracePeriod": grace_period
                            }
                        }
                    
                    if manifest:
                        manifest_yaml = yaml.dump(manifest)
                        manifest_json = json.dumps(manifest)
                        kind = manifest["kind"].lower() + "s"
                        
                        Kestra.outputs({
                            "manifest": manifest_yaml,
                            "manifest_json": manifest_json,
                            "kind": kind
                        })
                        print(f"‚úÖ Generated Chaos Mesh manifest for {chaos_type}", file=sys.stderr)
                        print(manifest_yaml, file=sys.stderr)
                    else:
                        print(f"‚ùå Unknown chaos type: {chaos_type}", file=sys.stderr)
                        sys.exit(1)

                - id: apply_chaos_mesh_experiment
                  type: io.kestra.plugin.core.http.Request
                  uri: "{{ inputs.kubernetes_api_url }}/apis/chaos-mesh.org/v1alpha1/namespaces/{{ inputs.chaos_namespace }}/{{ outputs.build_chaos_mesh_manifest.vars.kind }}"
                  method: POST
                  headers:
                    Authorization: "Bearer {{ inputs.kubernetes_token }}"
                    Content-Type: "application/json"
                  body: "{{ outputs.build_chaos_mesh_manifest.vars.manifest_json }}"
                  allowFailed: true

                - id: log_chaos_mesh_success
                  type: io.kestra.plugin.core.log.Log
                  message: "üå™Ô∏è Chaos Mesh: Applied {{ outputs.determine_chaos_config.vars.chaos_type }} experiment ({{ inputs.chaos_intensity }})"

              else:
                - id: log_chaos_mesh_mock
                  type: io.kestra.plugin.core.log.Log
                  message: "üå™Ô∏è [MOCK] Chaos Mesh: Would apply {{ outputs.determine_chaos_config.vars.chaos_type }} ({{ inputs.chaos_intensity }} intensity, {{ inputs.chaos_duration }} duration)"

          default:
            - id: log_unsupported_chaos
              type: io.kestra.plugin.core.log.Log
              message: "‚ö†Ô∏è Unsupported chaos method: {{ outputs.determine_chaos_config.vars.method }}"

  # PHASE 1 ‚Äî FETCH DEPLOYMENT HISTORY (GitHub)
  - id: fetch_deployment_history
    type: io.kestra.plugin.core.flow.If
    condition: "{{ inputs.github_token != 'mock' }}"
    then:
      - id: github_fetch_deployments
        type: io.kestra.plugin.core.http.Request
        uri: "https://api.github.com/repos/{{ inputs.deployments_repo }}/deployments"
        method: GET
        headers:
          Authorization: "token {{ inputs.github_token }}"
          Accept: "application/vnd.github.v3+json"
        allowFailed: true

      - id: parse_github_deployments
        type: io.kestra.plugin.scripts.python.Script
        docker:
          image: python:3.11-slim
        beforeCommands:
          - pip install kestra
        script: |
          import json
          import sys
          from kestra import Kestra
          
          try:
              deployments_raw = """{{ outputs.github_fetch_deployments.body }}"""
              deployments = json.loads(deployments_raw) if isinstance(deployments_raw, str) else deployments_raw
              
              # Extract last 5 deployments
              deployment_history = []
              for dep in deployments[:5]:
                  deployment_history.append({
                      "id": dep.get("id"),
                      "sha": dep.get("sha", "unknown")[:7],
                      "environment": dep.get("environment", "unknown"),
                      "created_at": dep.get("created_at", "unknown"),
                      "ref": dep.get("ref", "unknown")
                  })
              
              Kestra.outputs({
                  "deployment_history": json.dumps(deployment_history),
                  "latest_sha": deployment_history[0]["sha"] if deployment_history else "unknown",
                  "source": "github"
              })
              
              print(f"Fetched {len(deployment_history)} deployments from GitHub", file=sys.stderr)
              
          except Exception as e:
              print(f"GitHub fetch failed: {e}, using mock data", file=sys.stderr)
              mock_history = [
                  {"id": 101, "sha": "abc1234", "environment": "production", "created_at": "2025-12-12T10:00:00Z", "ref": "main"},
                  {"id": 100, "sha": "def5678", "environment": "production", "created_at": "2025-12-11T15:30:00Z", "ref": "main"}
              ]
              Kestra.outputs({
                  "deployment_history": json.dumps(mock_history),
                  "latest_sha": "abc1234",
                  "source": "mock"
              })
    else:
      - id: use_mock_deployments
        type: io.kestra.plugin.scripts.python.Script
        docker:
          image: python:3.11-slim
        beforeCommands:
          - pip install kestra
        script: |
          import json
          from kestra import Kestra
          
          mock_history = [
              {"id": 101, "sha": "abc1234", "environment": "production", "created_at": "2025-12-12T10:00:00Z", "ref": "main"},
              {"id": 100, "sha": "def5678", "environment": "production", "created_at": "2025-12-11T15:30:00Z", "ref": "main"},
              {"id": 99, "sha": "ghi9012", "environment": "production", "created_at": "2025-12-10T09:15:00Z", "ref": "main"}
          ]
          
          Kestra.outputs({
              "deployment_history": json.dumps(mock_history),
              "latest_sha": "abc1234",
              "source": "mock"
          })
          
          print("üì¶ Using mock deployment history", file=sys.stderr)

  # PHASE 1B ‚Äî CONSOLIDATE DEPLOYMENT DATA
  - id: consolidate_deployment_data
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    beforeCommands:
      - pip install kestra
    script: |
      import json
      import sys
      from kestra import Kestra

      # Read GitHub outputs safely (if they exist)
      gh_history = """{{ outputs.parse_github_deployments.deployment_history | default('') }}"""
      gh_source = """{{ outputs.parse_github_deployments.source | default('') }}"""

      if gh_history.strip():
          # GitHub returned real data
          Kestra.outputs({
              "deployment_history": gh_history,
              "source": gh_source or "github"
          })
          print("Using GitHub deployment history", file=sys.stderr)
      else:
          # Use mock fallback (task always exists when token == 'mock')
          mock_history = """{{ outputs.use_mock_deployments.deployment_history | default('') }}"""
          mock_source = """{{ outputs.use_mock_deployments.source | default('mock') }}"""

          Kestra.outputs({
              "deployment_history": mock_history,
              "source": mock_source
          })
          print("Using MOCK deployment history", file=sys.stderr)

  # PHASE 2 ‚Äî OBSERVE
  - id: collect_signals
    type: io.kestra.plugin.core.flow.Parallel
    tasks:

      - id: fetch_metrics
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.metrics_url }}"
        method: GET

      - id: fetch_logs
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.logs_url }}"
        method: GET

      - id: fetch_alerts
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.alerts_url }}"
        method: GET

  # PHASE 3 ‚Äî PARSE METRICS
  - id: parse_metrics
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    beforeCommands:
      - pip install kestra
    script: |
      import json
      import sys
      from kestra import Kestra
      
      try:
          raw_body = """{{ outputs.fetch_metrics.body }}"""
          
          if isinstance(raw_body, dict):
              metrics = raw_body
          elif isinstance(raw_body, str):
              metrics = json.loads(raw_body)
          else:
              raise ValueError(f"Unexpected type: {type(raw_body)}")
          
          m = metrics.get('metrics', {})
          
          error_rate = float(m.get('error_rate_percent', 0))
          p95_latency = float(m.get('p95_latency_ms', 0))
          p99_latency = float(m.get('p99_latency_ms', 0))
          median_latency = float(m.get('p50_latency_ms', 0))
          rps = float(m.get('requests_per_second', 0))
          cpu_usage = float(m.get('cpu_usage_percent', 0))
          memory_usage = float(m.get('memory_usage_percent', 0))
          deployment_id = int(metrics.get('deployment_id', 0))
          health_status = str(metrics.get('health_status', 'unknown'))
          
          Kestra.outputs({
              'error_rate': error_rate,
              'p95_latency': p95_latency,
              'p99_latency': p99_latency,
              'median_latency': median_latency,
              'rps': rps,
              'cpu_usage': cpu_usage,
              'memory_usage': memory_usage,
              'deployment_id': deployment_id,
              'health_status': health_status
          })
          
          print(f"  Metrics parsed successfully:", file=sys.stderr)
          print(f"  Error rate: {error_rate}%", file=sys.stderr)
          print(f"  P99 latency: {p99_latency}ms", file=sys.stderr)
          print(f"  CPU: {cpu_usage}%", file=sys.stderr)
          print(f"  Deployment: {deployment_id}", file=sys.stderr)
          print(f"  Health: {health_status}", file=sys.stderr)
          
      except Exception as e:
          print(f" ERROR parsing metrics: {e}", file=sys.stderr)
          import traceback
          traceback.print_exc(file=sys.stderr)
          
          Kestra.outputs({
              'error_rate': 0,
              'p95_latency': 0,
              'p99_latency': 0,
              'median_latency': 0,
              'rps': 0,
              'cpu_usage': 0,
              'memory_usage': 0,
              'deployment_id': 0,
              'health_status': 'unknown'
          })

  # PHASE 4 ‚Äî ANOMALY DETECTION
  - id: anomaly_detection
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    beforeCommands:
      - pip install kestra
    script: |
      from kestra import Kestra
      import sys
      
      anomalies = []
      error_rate = {{ outputs.parse_metrics.vars.error_rate }}
      cpu = {{ outputs.parse_metrics.vars.cpu_usage }}
      memory = {{ outputs.parse_metrics.vars.memory_usage }}
      p99 = {{ outputs.parse_metrics.vars.p99_latency }}
      p95 = {{ outputs.parse_metrics.vars.p95_latency }}
      
      # Detect anomalies based on thresholds
      if error_rate > 5:
          anomalies.append(f"ERROR_RATE_SPIKE: {error_rate}%")
      if cpu > 85:
          anomalies.append(f"CPU_SPIKE: {cpu}%")
      if memory > 85:
          anomalies.append(f"MEMORY_SPIKE: {memory}%")
      if p99 > 500:
          anomalies.append(f"P99_LATENCY_SPIKE: {p99}ms")
      if p95 > 300:
          anomalies.append(f"P95_LATENCY_SPIKE: {p95}ms")
      
      anomaly_summary = ", ".join(anomalies) if anomalies else "No anomalies detected"
      
      Kestra.outputs({
          "anomalies": anomalies,
          "count": len(anomalies),
          "summary": anomaly_summary
      })
      
      print(f"   Anomaly Detection: {anomaly_summary}", file=sys.stderr)
      print(f"   Total anomalies: {len(anomalies)}", file=sys.stderr)

  # PHASE 5 ‚Äî AI ANALYSIS
  - id: ai_incident_analysis
    type: io.kestra.plugin.openai.ChatCompletion
    description: AI decision engine with deployment history context
    apiKey: "{{ secret('OPENAI_API_KEY') }}"
    model: "gpt-4o-mini"
    temperature: 0.3
    messages:
      - role: system
        content: |
          You MUST respond with exactly this JSON structure:

          {
            "severity": "P0" | "P1" | "P2" | "P3",
            "confidence_score": 85,
            "incident_summary": "string",
            "root_cause_hypothesis": "string",
            "deployment_to_rollback": 100,
            "recommended_action": "rollback" | "monitor" | "escalate",
            "pagerduty_priority": "P1" | "P2" | "P3" | "P4" | "P5"
          }

          Rules:
          - confidence_score: 0-100, how confident you are in your analysis
          - Use "rollback" only if you're confident the deployment caused the issue
          - Use "monitor" for unclear situations
          - Use "escalate" for complex issues needing human intervention
          - Error rate > 10% is P0 critical
          - P99 latency > 800ms is P0 critical
          - Error rate > 5% or P99 > 500ms is P1
          - CPU/Memory > 90% is concerning
          - Consider deployment history when suggesting rollbacks
          - pagerduty_priority should align with severity (P0‚ÜíP1, P1‚ÜíP2, etc.)
          
          NO OTHER FIELDS. NO nested objects. NO markdown. ONLY this JSON object.

      - role: user
        content: |
          SERVICE: {{ inputs.service_name }}
          ENVIRONMENT: {{ inputs.environment }}
          TIMESTAMP: {{ execution.startDate }}

          DEPLOYMENT HISTORY:
          {{ outputs.consolidate_deployment_data.vars.deployment_history }}
          (Source: {{ outputs.consolidate_deployment_data.vars.source }})

          CURRENT METRICS:
          - Error Rate: {{ outputs.parse_metrics.vars.error_rate }}%
          - P95 Latency: {{ outputs.parse_metrics.vars.p95_latency }}ms
          - P99 Latency: {{ outputs.parse_metrics.vars.p99_latency }}ms
          - Median Latency: {{ outputs.parse_metrics.vars.median_latency }}ms
          - Requests/sec: {{ outputs.parse_metrics.vars.rps }}
          - CPU Usage: {{ outputs.parse_metrics.vars.cpu_usage }}%
          - Memory Usage: {{ outputs.parse_metrics.vars.memory_usage }}%
          - Current Deployment: {{ outputs.parse_metrics.vars.deployment_id }}
          - Health Status: {{ outputs.parse_metrics.vars.health_status }}

          DETECTED ANOMALIES ({{ outputs.anomaly_detection.vars.count }}):
          {{ outputs.anomaly_detection.vars.summary }}

          LOGS:
          {{ outputs.fetch_logs.body }}

          ALERTS:
          {{ outputs.fetch_alerts.body }}

          MIN_CONFIDENCE_THRESHOLD: {{ inputs.min_confidence_for_rollback }}

          Analyze the incident considering metrics, anomalies, and deployment history.
          If rollback is needed, suggest the previous deployment from the history.
          Respond with ONLY valid JSON.

  # PHASE 6 ‚Äî PARSE AI OUTPUT
  - id: parse_ai_decision
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    beforeCommands:
      - pip install kestra
    script: |
      import json
      import sys
      from kestra import Kestra

      raw_content = """{{ outputs.ai_incident_analysis.choices[0].message.content }}"""
      
      content = raw_content.strip()
      
      # Remove markdown code blocks if present
      if content.startswith('```'):
          content = content[content.find('\n')+1:]
      if content.endswith('```'):
          content = content[:content.rfind('\n')]
      
      content = content.strip()
      
      try:
          data = json.loads(content)
      except json.JSONDecodeError as e:
          print(f"ERROR: Failed to parse JSON: {e}", file=sys.stderr)
          print(f"Raw content: {content}", file=sys.stderr)
          sys.exit(1)
      
      Kestra.outputs({
          'severity': data.get('severity', 'UNKNOWN'),
          'confidence_score': data.get('confidence_score', 0),
          'incident_summary': data.get('incident_summary', 'No summary'),
          'root_cause_hypothesis': data.get('root_cause_hypothesis', 'Unknown'),
          'deployment_to_rollback': data.get('deployment_to_rollback', 100),
          'recommended_action': data.get('recommended_action', 'monitor'),
          'pagerduty_priority': data.get('pagerduty_priority', 'P3')
      })
      
      print(json.dumps(data, indent=2), file=sys.stderr)

  # PHASE 8 ‚Äî CONFIDENCE CHECK & ESCALATION
  - id: check_confidence_threshold
    type: io.kestra.plugin.core.flow.If
    condition: "{{ outputs.parse_ai_decision.vars.confidence_score < 70 }}"
    then:
      - id: escalate_to_humans
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.slack_webhook }}"
        method: POST
        contentType: application/json
        body: |
          {
            "text": "<!channel> üö® AI Confidence Low ({{ outputs.parse_ai_decision.vars.confidence_score }}%) - Human Review Required",
            "blocks": [
              {
                "type": "section",
                "text": {
                  "type": "mrkdwn",
                  "text": "*‚ö†Ô∏è LOW CONFIDENCE INCIDENT - HUMAN REVIEW NEEDED*\n\nThe AI is only {{ outputs.parse_ai_decision.vars.confidence_score }}% confident. Manual review required before taking action.\n\n*Incident Summary:*\n{{ outputs.parse_ai_decision.vars.incident_summary }}\n\n*Root Cause Hypothesis:*\n{{ outputs.parse_ai_decision.vars.root_cause_hypothesis }}\n\n*Detected Anomalies:*\n{{ outputs.anomaly_detection.vars.summary }}"
                }
              },
              {
                "type": "section",
                "fields": [
                  {
                    "type": "mrkdwn",
                    "text": "*Service:*\n{{ inputs.service_name }}"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*Deployment:*\n{{ outputs.parse_metrics.vars.deployment_id }}"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*Error Rate:*\n{{ outputs.parse_metrics.vars.error_rate }}%"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*P99 Latency:*\n{{ outputs.parse_metrics.vars.p99_latency }}ms"
                  }
                ]
              }
            ]
          }

  # PHASE 9 ‚Äî ROUTE BY SEVERITY
  - id: route_by_severity
    type: io.kestra.plugin.core.flow.Switch
    value: "{{ outputs.parse_ai_decision.vars.severity }}"
    cases:

      P0:
        - id: handle_p0
          type: io.kestra.plugin.core.flow.Sequential
          tasks:
            - id: evaluate_rollback_decision
              type: io.kestra.plugin.core.flow.If
              condition: "{{ outputs.parse_ai_decision.vars.confidence_score >= inputs.min_confidence_for_rollback and outputs.parse_ai_decision.vars.recommended_action == 'rollback' }}"
              then:
                - id: check_github_token
                  type: io.kestra.plugin.core.flow.If
                  condition: "{{ inputs.github_token != 'mock' }}"
                  then:
                    - id: trigger_github_action
                      type: io.kestra.plugin.core.http.Request
                      uri: "https://api.github.com/repos/{{ inputs.deployments_repo }}/actions/workflows/rollback.yml/dispatches"
                      method: POST
                      headers:
                        Authorization: "token {{ inputs.github_token }}"
                        Accept: "application/vnd.github+json"
                        Content-Type: "application/json"
                      body: |
                        {
                          "ref": "master",
                          "inputs": {
                            "rollback_to": "{{ outputs.parse_ai_decision.vars.deployment_to_rollback }}",
                            "service": "{{ inputs.service_name }}",
                            "environment": "{{ inputs.environment }}",
                            "execution_id": "{{ execution.id }}"
                          }
                        }

                    - id: log_rollback_success
                      type: io.kestra.plugin.core.log.Log
                      message: "‚úî GitHub Action triggered for rollback ‚Üí {{ outputs.parse_ai_decision.vars.deployment_to_rollback }}"
                  else:
                    - id: log_rollback_mock
                      type: io.kestra.plugin.core.log.Log
                      message: "üîÑ [MOCK] Rollback would trigger ‚Üí deployment {{ outputs.parse_ai_decision.vars.deployment_to_rollback }}"
              else:
                - id: skip_rollback_log
                  type: io.kestra.plugin.core.log.Log
                  message: "‚è≠Ô∏è Rollback skipped - Confidence {{ outputs.parse_ai_decision.vars.confidence_score }}% or action '{{ outputs.parse_ai_decision.vars.recommended_action }}'"

            - id: trigger_pagerduty
              type: io.kestra.plugin.core.flow.If
              condition: "{{ inputs.pagerduty_token != 'mock' }}"
              then:
                - id: pagerduty_real_alert
                  type: io.kestra.plugin.core.http.Request
                  uri: https://events.pagerduty.com/v2/enqueue
                  method: POST
                  contentType: application/json
                  body: |
                    {
                      "routing_key": "{{ inputs.pagerduty_token }}",
                      "event_action": "trigger",
                      "payload": {
                        "summary": "P0 CRITICAL: {{ inputs.service_name }} ({{ inputs.environment }}) - {{ outputs.parse_ai_decision.vars.incident_summary }}",
                        "severity": "critical",
                        "source": "{{ inputs.service_name }}",
                        "component": "{{ inputs.environment }}",
                        "custom_details": {
                          "incident_summary": "{{ outputs.parse_ai_decision.vars.incident_summary }}",
                          "root_cause": "{{ outputs.parse_ai_decision.vars.root_cause_hypothesis }}",
                          "error_rate": "{{ outputs.parse_metrics.vars.error_rate }}%",
                          "p95_latency": "{{ outputs.parse_metrics.vars.p95_latency }}ms",
                          "p99_latency": "{{ outputs.parse_metrics.vars.p99_latency }}ms",
                          "cpu_usage": "{{ outputs.parse_metrics.vars.cpu_usage }}%",
                          "confidence": "{{ outputs.parse_ai_decision.vars.confidence_score }}%",
                          "anomalies": "{{ outputs.anomaly_detection.vars.summary }}",
                          "action_taken": "{{ outputs.parse_ai_decision.vars.recommended_action }}",
                          "deployment_id": "{{ outputs.parse_metrics.vars.deployment_id }}",
                          "execution_id": "{{ execution.id }}"
                        }
                      }
                    }
              else:
                - id: pagerduty_mock_alert
                  type: io.kestra.plugin.core.log.Log
                  message: "üö® [MOCK] PagerDuty alert triggered - P0 CRITICAL: {{ inputs.service_name }} - {{ outputs.parse_ai_decision.vars.incident_summary }}"

            - id: slack_notify_p0
              type: io.kestra.plugin.core.http.Request
              uri: "{{ inputs.slack_webhook }}"
              method: POST
              contentType: application/json
              body: |
                {
                  "blocks": [
                    {
                      "type": "header",
                      "text": {
                        "type": "plain_text",
                        "text": "üî¥ P0 CRITICAL INCIDENT",
                        "emoji": true
                      }
                    },
                    {
                      "type": "section",
                      "fields": [
                        {
                          "type": "mrkdwn",
                          "text": "*Service:*\n{{ inputs.service_name }}"
                        },
                        {
                          "type": "mrkdwn",
                          "text": "*Environment:*\n{{ inputs.environment }}"
                        },
                        {
                          "type": "mrkdwn",
                          "text": "*Severity:*\nP0"
                        },
                        {
                          "type": "mrkdwn",
                          "text": "*Confidence:*\n{{ outputs.parse_ai_decision.vars.confidence_score }}%"
                        }
                      ]
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Incident Summary:*\n{{ outputs.parse_ai_decision.vars.incident_summary }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Root Cause Hypothesis:*\n{{ outputs.parse_ai_decision.vars.root_cause_hypothesis }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Detected Anomalies:*\n{{ outputs.anomaly_detection.vars.summary }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Current Metrics:*\n‚Ä¢ Error Rate: {{ outputs.parse_metrics.vars.error_rate }}%\n‚Ä¢ P95 Latency: {{ outputs.parse_metrics.vars.p95_latency }}ms\n‚Ä¢ P99 Latency: {{ outputs.parse_metrics.vars.p99_latency }}ms\n‚Ä¢ CPU Usage: {{ outputs.parse_metrics.vars.cpu_usage }}%\n‚Ä¢ Deployment: {{ outputs.parse_metrics.vars.deployment_id }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Action Taken:*\n{{ outputs.parse_ai_decision.vars.recommended_action }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Deployment Source:*\n{{ outputs.consolidate_deployment_data.vars.source }}"
                      }
                    },
                    {
                      "type": "divider"
                    },
                    {
                      "type": "context",
                      "elements": [
                        {
                          "type": "mrkdwn",
                          "text": "üö® PagerDuty alert triggered | Execution ID: {{ execution.id }} | Started: {{ execution.startDate }}"
                        }
                      ]
                    }
                  ]
                }

      P1:
        - id: handle_p1
          type: io.kestra.plugin.core.flow.Sequential
          tasks:
            - id: log_p1_incident
              type: io.kestra.plugin.core.log.Log
              message: "üü† P1 MAJOR incident logged: {{ outputs.parse_ai_decision.vars.incident_summary }}"

            - id: trigger_pagerduty_p1
              type: io.kestra.plugin.core.flow.If
              condition: "{{ inputs.pagerduty_token != 'mock' }}"
              then:
                - id: pagerduty_p1_alert
                  type: io.kestra.plugin.core.http.Request
                  uri: https://events.pagerduty.com/v2/enqueue
                  method: POST
                  contentType: application/json
                  body: |
                    {
                      "routing_key": "{{ inputs.pagerduty_token }}",
                      "event_action": "trigger",
                      "payload": {
                        "summary": "P1 MAJOR: {{ inputs.service_name }} ({{ inputs.environment }}) - {{ outputs.parse_ai_decision.vars.incident_summary }}",
                        "severity": "error",
                        "source": "{{ inputs.service_name }}",
                        "component": "{{ inputs.environment }}",
                        "custom_details": {
                          "incident_summary": "{{ outputs.parse_ai_decision.vars.incident_summary }}",
                          "root_cause": "{{ outputs.parse_ai_decision.vars.root_cause_hypothesis }}",
                          "error_rate": "{{ outputs.parse_metrics.vars.error_rate }}%",
                          "p95_latency": "{{ outputs.parse_metrics.vars.p95_latency }}ms",
                          "p99_latency": "{{ outputs.parse_metrics.vars.p99_latency }}ms",
                          "confidence": "{{ outputs.parse_ai_decision.vars.confidence_score }}%",
                          "anomalies": "{{ outputs.anomaly_detection.vars.summary }}",
                          "deployment_id": "{{ outputs.parse_metrics.vars.deployment_id }}",
                          "execution_id": "{{ execution.id }}"
                        }
                      }
                    }
              else:
                - id: pagerduty_p1_mock
                  type: io.kestra.plugin.core.log.Log
                  message: "üü† [MOCK] PagerDuty alert triggered - P1 MAJOR: {{ inputs.service_name }} - {{ outputs.parse_ai_decision.vars.incident_summary }}"

            - id: slack_notify_p1
              type: io.kestra.plugin.core.http.Request
              uri: "{{ inputs.slack_webhook }}"
              method: POST
              contentType: application/json
              body: |
                {
                  "blocks": [
                    {
                      "type": "header",
                      "text": {
                        "type": "plain_text",
                        "text": "üü† P1 MAJOR INCIDENT",
                        "emoji": true
                      }
                    },
                    {
                      "type": "section",
                      "fields": [
                        {
                          "type": "mrkdwn",
                          "text": "*Service:*\n{{ inputs.service_name }}"
                        },
                        {
                          "type": "mrkdwn",
                          "text": "*Environment:*\n{{ inputs.environment }}"
                        },
                        {
                          "type": "mrkdwn",
                          "text": "*Severity:*\nP1"
                        },
                        {
                          "type": "mrkdwn",
                          "text": "*Confidence:*\n{{ outputs.parse_ai_decision.vars.confidence_score }}%"
                        }
                      ]
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Incident Summary:*\n{{ outputs.parse_ai_decision.vars.incident_summary }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Root Cause Hypothesis:*\n{{ outputs.parse_ai_decision.vars.root_cause_hypothesis }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Detected Anomalies:*\n{{ outputs.anomaly_detection.vars.summary }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Current Metrics:*\n‚Ä¢ Error Rate: {{ outputs.parse_metrics.vars.error_rate }}%\n‚Ä¢ P95 Latency: {{ outputs.parse_metrics.vars.p95_latency }}ms\n‚Ä¢ P99 Latency: {{ outputs.parse_metrics.vars.p99_latency }}ms\n‚Ä¢ CPU Usage: {{ outputs.parse_metrics.vars.cpu_usage }}%"
                      }
                    },
                    {
                      "type": "divider"
                    },
                    {
                      "type": "context",
                      "elements": [
                        {
                          "type": "mrkdwn",
                          "text": "üö® PagerDuty alert triggered | No automatic rollback | Execution ID: {{ execution.id }}"
                        }
                      ]
                    }
                  ]
                }

      P2:
        - id: handle_p2
          type: io.kestra.plugin.core.flow.Sequential
          tasks:
            - id: log_p2_incident
              type: io.kestra.plugin.core.log.Log
              message: "üü° P2 incident logged: {{ outputs.parse_ai_decision.vars.incident_summary }}"
        
            - id: notify_p2_slack
              type: io.kestra.plugin.core.http.Request
              uri: "{{ inputs.slack_webhook }}"
              method: POST
              contentType: application/json
              body: |
                {
                  "blocks": [
                    {
                      "type": "header",
                      "text": {
                        "type": "plain_text",
                        "text": "üü° P2 Incident",
                        "emoji": true
                      }
                    },
                    {
                      "type": "section",
                      "fields": [
                        {
                          "type": "mrkdwn",
                          "text": "*Service:*\n{{ inputs.service_name }}"
                        },
                        {
                          "type": "mrkdwn",
                          "text": "*Confidence:*\n{{ outputs.parse_ai_decision.vars.confidence_score }}%"
                        }
                      ]
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Summary:*\n{{ outputs.parse_ai_decision.vars.incident_summary }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Metrics:*\nError Rate: {{ outputs.parse_metrics.vars.error_rate }}% | P99: {{ outputs.parse_metrics.vars.p99_latency }}ms"
                      }
                    },
                    {
                      "type": "context",
                      "elements": [
                        {
                          "type": "mrkdwn",
                          "text": "üìä Monitored during business hours ‚Ä¢ No immediate action required"
                        }
                      ]
                    }
                  ]
                }

      P3:
        - id: handle_p3
          type: io.kestra.plugin.core.log.Log
          message: "P3 normal state - minor issue logged: {{ outputs.parse_ai_decision.vars.incident_summary }}"

      default:
        - id: handle_invalid_ai_output
          type: io.kestra.plugin.core.log.Log
          message: "Invalid AI output or missing severity."

  # PHASE 10 ‚Äî HEALTH CHECK AFTER REMEDIATION (Skip for P3)
  - id: post_remediation_health_check
    type: io.kestra.plugin.core.flow.If
    condition: "{{ outputs.parse_ai_decision.vars.severity != 'P3' }}"
    then:
      - id: wait_for_stabilization
        type: io.kestra.plugin.core.flow.Sleep
        duration: PT10S

      - id: fetch_post_metrics
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.metrics_url }}"
        method: GET

      - id: fetch_post_alerts
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.alerts_url }}"
        method: GET

      - id: parse_post_metrics
        type: io.kestra.plugin.scripts.python.Script
        docker:
          image: python:3.11-slim
        beforeCommands:
          - pip install kestra
        script: |
          import json
          import sys
          from kestra import Kestra
          
          try:
              raw_body = """{{ outputs.fetch_post_metrics.body }}"""
              
              if isinstance(raw_body, dict):
                  metrics = raw_body
              elif isinstance(raw_body, str):
                  metrics = json.loads(raw_body)
              else:
                  raise ValueError(f"Unexpected type: {type(raw_body)}")
              
              m = metrics.get('metrics', {})
              
              Kestra.outputs({
                  'error_rate': float(m.get('error_rate_percent', 0)),
                  'p99_latency': float(m.get('p99_latency_ms', 0)),
                  'deployment_id': int(metrics.get('deployment_id', 0)),
                  'health_status': str(metrics.get('health_status', 'unknown'))
              })
              
              print(f" Post-metrics parsed: deployment={metrics.get('deployment_id')}, error={m.get('error_rate_percent')}%", file=sys.stderr)
              
          except Exception as e:
              print(f" Error parsing post-metrics: {e}", file=sys.stderr)
              Kestra.outputs({
                  'error_rate': 0,
                  'p99_latency': 0,
                  'deployment_id': 0,
                  'health_status': 'unknown'
              })

      - id: compare_health_status
        type: io.kestra.plugin.scripts.python.Script
        docker:
          image: python:3.11-slim
        beforeCommands:
          - pip install kestra
        script: |
          import json
          import sys
          from kestra import Kestra

          before_error_rate = {{ outputs.parse_metrics.vars.error_rate }}
          after_error_rate = {{ outputs.parse_post_metrics.vars.error_rate }}
          
          before_p99 = {{ outputs.parse_metrics.vars.p99_latency }}
          after_p99 = {{ outputs.parse_post_metrics.vars.p99_latency }}
          
          before_deployment = {{ outputs.parse_metrics.vars.deployment_id }}
          after_deployment = {{ outputs.parse_post_metrics.vars.deployment_id }}
          
          if before_error_rate > 0:
              error_improvement_pct = ((before_error_rate - after_error_rate) / before_error_rate) * 100
          else:
              error_improvement_pct = 0
          
          if before_p99 > 0:
              latency_improvement_pct = ((before_p99 - after_p99) / before_p99) * 100
          else:
              latency_improvement_pct = 0
          
          if after_error_rate < 1 and after_p99 < 200:
              health_status = "FULLY_RECOVERED"
          elif after_error_rate < before_error_rate * 0.5 and latency_improvement_pct > 30:
              health_status = "SIGNIFICANT_IMPROVEMENT"
          elif after_error_rate < before_error_rate or after_p99 < before_p99:
              health_status = "IMPROVING"
          elif after_error_rate == before_error_rate and after_p99 == before_p99:
              health_status = "STABLE"
          else:
              health_status = "DEGRADED"
          
          Kestra.outputs({
              'health_status': health_status,
              'before_error_rate': before_error_rate,
              'after_error_rate': after_error_rate,
              'error_improvement_pct': round(error_improvement_pct, 2),
              'before_p99_latency': before_p99,
              'after_p99_latency': after_p99,
              'latency_improvement_pct': round(latency_improvement_pct, 2),
              'deployment_changed': before_deployment != after_deployment
          })
          
          print(f"Health check complete: {health_status}", file=sys.stderr)
          print(f"Error rate: {before_error_rate}% -> {after_error_rate}% ({error_improvement_pct:.2f}% improvement)", file=sys.stderr)
          print(f"P99 latency: {before_p99}ms -> {after_p99}ms ({latency_improvement_pct:.2f}% improvement)", file=sys.stderr)
          print(f"Deployment: {before_deployment} -> {after_deployment}", file=sys.stderr)

      - id: notify_health_status
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.slack_webhook }}"
        method: POST
        contentType: application/json
        body: |
          {
            "blocks": [
              {
                "type": "header",
                "text": {
                  "type": "plain_text",
                  "text": "üìä Post-Remediation Health Check",
                  "emoji": true
                }
              },
              {
                "type": "section",
                "fields": [
                  {
                    "type": "mrkdwn",
                    "text": "*Status:*\n{{ outputs.compare_health_status.vars.health_status }}"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*Error Rate:*\n{{ outputs.compare_health_status.vars.before_error_rate }}% ‚Üí {{ outputs.compare_health_status.vars.after_error_rate }}%"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*P99 Latency:*\n{{ outputs.compare_health_status.vars.before_p99_latency }}ms ‚Üí {{ outputs.compare_health_status.vars.after_p99_latency }}ms"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*Improvement:*\nError: {{ outputs.compare_health_status.vars.error_improvement_pct }}%\nLatency: {{ outputs.compare_health_status.vars.latency_improvement_pct }}%"
                  }
                ]
              },
              {
                "type": "context",
                "elements": [
                  {
                    "type": "mrkdwn",
                    "text": "Incident: {{ outputs.parse_ai_decision.vars.severity }} | Execution: {{ execution.id }}"
                  }
                ]
              }
            ]
          }

  # PHASE 11 ‚Äî SAVE INCIDENT SNAPSHOT
  - id: save_incident_snapshot
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    beforeCommands:
      - pip install kestra
    script: |
      import json
      from kestra import Kestra

      # Convert booleans safely
      chaos_mode = "{{ inputs.chaos_mode }}".lower() == "true"
      pagerduty_used = "{{ inputs.pagerduty_token }}" != "mock"
      severity = "{{ outputs.parse_ai_decision.vars.severity }}"
      
      # Check if health check ran (only for P0/P1/P2)
      health_check_ran = severity != "P3"
      
      snapshot = {
          "execution_id": "{{ execution.id }}",
          "timestamp": "{{ execution.startDate }}",
          "service": "{{ inputs.service_name }}",
          "environment": "{{ inputs.environment }}",
          "chaos_mode": chaos_mode,
          "severity": severity,
          "confidence": float("{{ outputs.parse_ai_decision.vars.confidence_score }}"),
          "summary": "{{ outputs.parse_ai_decision.vars.incident_summary }}",
          "root_cause": "{{ outputs.parse_ai_decision.vars.root_cause_hypothesis }}",
          "action_taken": "{{ outputs.parse_ai_decision.vars.recommended_action }}",
          "deployment_source": "{{ outputs.consolidate_deployment_data.vars.source }}",
          "pagerduty_priority": "{{ outputs.parse_ai_decision.vars.pagerduty_priority }}",

          "anomalies": {
              "count": int("{{ outputs.anomaly_detection.vars.count }}"),
              "summary": "{{ outputs.anomaly_detection.vars.summary }}"
          },

          "metrics": {
              "before": {
                  "error_rate": float("{{ outputs.parse_metrics.vars.error_rate }}"),
                  "p99_latency": float("{{ outputs.parse_metrics.vars.p99_latency }}"),
                  "p95_latency": float("{{ outputs.parse_metrics.vars.p95_latency }}"),
                  "cpu_usage": float("{{ outputs.parse_metrics.vars.cpu_usage }}"),
                  "memory_usage": float("{{ outputs.parse_metrics.vars.memory_usage }}"),
                  "deployment_id": int("{{ outputs.parse_metrics.vars.deployment_id }}"),
                  "health_status": "{{ outputs.parse_metrics.vars.health_status }}"
              }
          },

          "integrations": {
              "github": "{{ outputs.consolidate_deployment_data.vars.source }}",
              "pagerduty": pagerduty_used,
              "slack": True
          }
      }
      
      # Only include health check data if it ran (P0/P1/P2)
      if health_check_ran:
          deployment_changed = "{{ outputs.compare_health_status.vars.deployment_changed | default('false') }}".lower() == "true"
          
          snapshot["metrics"]["after"] = {
              "error_rate": float("{{ outputs.compare_health_status.vars.after_error_rate | default('0') }}"),
              "p99_latency": float("{{ outputs.compare_health_status.vars.after_p99_latency | default('0') }}"),
              "deployment_id": int("{{ outputs.parse_post_metrics.vars.deployment_id | default('0') }}"),
              "health_status": "{{ outputs.parse_post_metrics.vars.health_status | default('unknown') }}"
          }
          
          snapshot["metrics"]["improvement"] = {
              "error_rate_pct": float("{{ outputs.compare_health_status.vars.error_improvement_pct | default('0') }}"),
              "latency_pct": float("{{ outputs.compare_health_status.vars.latency_improvement_pct | default('0') }}")
          }
          
          snapshot["health_status"] = "{{ outputs.compare_health_status.vars.health_status | default('unknown') }}"
          snapshot["deployment_changed"] = deployment_changed
      else:
          # P3 - no health check data
          snapshot["health_status"] = "not_checked"
          snapshot["deployment_changed"] = False

      Kestra.outputs(snapshot)

      print("=" * 60)
      print("INCIDENT SNAPSHOT SAVED")
      print(json.dumps(snapshot, indent=2))
      print("=" * 60)

triggers:
  - id: scheduled_monitor
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "*/5 * * * *"
