id: ai-incident-commander
namespace: hackathon.sre

description: |
  AI-powered autonomous incident response system that monitors, analyzes, and remediates production incidents.
  Features: Anomaly detection, chaos engineering mode, autonomous rollback, health monitoring, incident history.
  Integrations: GitHub (deployment tracking), PagerDuty (incident management), Slack (notifications).

inputs:
  - id: github_token
    type: STRING
    required: true
    defaults: "mock"
    description: "GitHub token for deployment history. Use 'mock' for demo mode."

  - id: pagerduty_token
    type: STRING
    required: true
    defaults: "mock"
    description: "PagerDuty API token. Use 'mock' for demo mode."

  - id: slack_webhook
    type: STRING
    required: true
    defaults: "https://hooks.slack.com/services/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"
    description: "Enter your own slack webhook here to recieve messages."

  - id: deployments_repo
    type: STRING
    defaults: "demo/service"
    description: "GitHub repo in format 'owner/repo'"

  - id: service_name
    type: STRING
    defaults: "payment-api"

  - id: service_url
    type: STRING
    defaults: "http://payment-api:3000"

  - id: environment
    type: STRING
    defaults: "production"

  - id: metrics_url
    type: STRING
    defaults: "http://payment-api:3000/metrics/json"

  - id: logs_url
    type: STRING
    defaults: "http://payment-api:3000/logs"

  - id: alerts_url
    type: STRING
    defaults: "http://payment-api:3000/alerts"

  - id: min_confidence_for_rollback
    type: INT
    defaults: 80

  - id: enable_auto_remediation
    type: BOOLEAN
    defaults: true

  - id: chaos_mode
    type: BOOLEAN
    defaults: false
    description: "Enable chaos injection for testing (injects 600ms latency)"

  - id: pagerduty_service_id
    type: STRING
    defaults: "PSERVICE1"
    description: "PagerDuty service ID for incident creation"

  - id: pagerduty_escalation_policy
    type: STRING
    defaults: "PEPOLICY1"
    description: "PagerDuty escalation policy ID"

tasks:
  # PHASE 0 â€” CHAOS INJECTION (OPTIONAL)
  - id: chaos_inject
    type: io.kestra.plugin.core.flow.If
    condition: "{{ inputs.chaos_mode }}"
    then:
      - id: inject_latency
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.service_url }}/chaos/latency"
        method: POST
        contentType: application/json
        body: |
          {
            "latency_ms": 600,
            "duration_seconds": 30,
            "reason": "Chaos testing from execution {{ execution.id }}"
          }

      - id: log_chaos_injection
        type: io.kestra.plugin.core.log.Log
        message: "Chaos mode: Injected 600ms latency for 30s"

  # PHASE 1 â€” FETCH DEPLOYMENT HISTORY (GitHub)
  - id: fetch_deployment_history
    type: io.kestra.plugin.core.flow.If
    condition: "{{ inputs.github_token != 'mock' }}"
    then:
      - id: github_fetch_deployments
        type: io.kestra.plugin.core.http.Request
        uri: "https://api.github.com/repos/{{ inputs.deployments_repo }}/deployments"
        method: GET
        headers:
          Authorization: "token {{ inputs.github_token }}"
          Accept: "application/vnd.github.v3+json"
        allowFailed: true

      - id: parse_github_deployments
        type: io.kestra.plugin.scripts.python.Script
        docker:
          image: python:3.11-slim
        beforeCommands:
          - pip install kestra
        script: |
          import json
          import sys
          from kestra import Kestra

          try:
              deployments_raw = """{{ outputs.github_fetch_deployments.body }}"""
              deployments = json.loads(deployments_raw) if isinstance(deployments_raw, str) else deployments_raw
              
              # Extract last 5 deployments
              deployment_history = []
              for dep in deployments[:5]:
                  deployment_history.append({
                      "id": dep.get("id"),
                      "sha": dep.get("sha", "unknown")[:7],
                      "environment": dep.get("environment", "unknown"),
                      "created_at": dep.get("created_at", "unknown"),
                      "ref": dep.get("ref", "unknown")
                  })
              
              Kestra.outputs({
                  "deployment_history": json.dumps(deployment_history),
                  "latest_sha": deployment_history[0]["sha"] if deployment_history else "unknown",
                  "source": "github"
              })
              
              print(f"Fetched {len(deployment_history)} deployments from GitHub", file=sys.stderr)
              
          except Exception as e:
              print(f"GitHub fetch failed: {e}, using mock data", file=sys.stderr)
              mock_history = [
                  {"id": 101, "sha": "abc1234", "environment": "production", "created_at": "2025-12-12T10:00:00Z", "ref": "main"},
                  {"id": 100, "sha": "def5678", "environment": "production", "created_at": "2025-12-11T15:30:00Z", "ref": "main"}
              ]
              Kestra.outputs({
                  "deployment_history": json.dumps(mock_history),
                  "latest_sha": "abc1234",
                  "source": "mock"
              })
    else:
      - id: use_mock_deployments
        type: io.kestra.plugin.scripts.python.Script
        docker:
          image: python:3.11-slim
        beforeCommands:
          - pip install kestra
        script: |
          import json
          from kestra import Kestra

          mock_history = [
              {"id": 101, "sha": "abc1234", "environment": "production", "created_at": "2025-12-12T10:00:00Z", "ref": "main"},
              {"id": 100, "sha": "def5678", "environment": "production", "created_at": "2025-12-11T15:30:00Z", "ref": "main"},
              {"id": 99, "sha": "ghi9012", "environment": "production", "created_at": "2025-12-10T09:15:00Z", "ref": "main"}
          ]

          Kestra.outputs({
              "deployment_history": json.dumps(mock_history),
              "latest_sha": "abc1234",
              "source": "mock"
          })

          print("ðŸ“¦ Using mock deployment history", file=sys.stderr)

  # PHASE 1B â€” CONSOLIDATE DEPLOYMENT DATA
  - id: consolidate_deployment_data
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    beforeCommands:
      - pip install kestra
    script: |
      import json
      import sys
      from kestra import Kestra

      # Read GitHub outputs safely (if they exist)
      gh_history = """{{ outputs.parse_github_deployments.deployment_history | default('') }}"""
      gh_source = """{{ outputs.parse_github_deployments.source | default('') }}"""

      if gh_history.strip():
          # GitHub returned real data
          Kestra.outputs({
              "deployment_history": gh_history,
              "source": gh_source or "github"
          })
          print("Using GitHub deployment history", file=sys.stderr)
      else:
          # Use mock fallback (task always exists when token == 'mock')
          mock_history = """{{ outputs.use_mock_deployments.deployment_history | default('') }}"""
          mock_source = """{{ outputs.use_mock_deployments.source | default('mock') }}"""

          Kestra.outputs({
              "deployment_history": mock_history,
              "source": mock_source
          })
          print("Using MOCK deployment history", file=sys.stderr)

  # PHASE 2 â€” OBSERVE
  - id: collect_signals
    type: io.kestra.plugin.core.flow.Parallel
    tasks:
      - id: fetch_metrics
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.metrics_url }}"
        method: GET

      - id: fetch_logs
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.logs_url }}"
        method: GET

      - id: fetch_alerts
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.alerts_url }}"
        method: GET

  # PHASE 3 â€” PARSE METRICS
  - id: parse_metrics
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    beforeCommands:
      - pip install kestra
    script: |
      import json
      import sys
      from kestra import Kestra

      try:
          raw_body = """{{ outputs.fetch_metrics.body }}"""
          
          if isinstance(raw_body, dict):
              metrics = raw_body
          elif isinstance(raw_body, str):
              metrics = json.loads(raw_body)
          else:
              raise ValueError(f"Unexpected type: {type(raw_body)}")
          
          m = metrics.get('metrics', {})
          
          error_rate = float(m.get('error_rate_percent', 0))
          p95_latency = float(m.get('p95_latency_ms', 0))
          p99_latency = float(m.get('p99_latency_ms', 0))
          median_latency = float(m.get('p50_latency_ms', 0))
          rps = float(m.get('requests_per_second', 0))
          cpu_usage = float(m.get('cpu_usage_percent', 0))
          memory_usage = float(m.get('memory_usage_percent', 0))
          deployment_id = int(metrics.get('deployment_id', 0))
          health_status = str(metrics.get('health_status', 'unknown'))
          
          Kestra.outputs({
              'error_rate': error_rate,
              'p95_latency': p95_latency,
              'p99_latency': p99_latency,
              'median_latency': median_latency,
              'rps': rps,
              'cpu_usage': cpu_usage,
              'memory_usage': memory_usage,
              'deployment_id': deployment_id,
              'health_status': health_status
          })
          
          print(f"  Metrics parsed successfully:", file=sys.stderr)
          print(f"  Error rate: {error_rate}%", file=sys.stderr)
          print(f"  P99 latency: {p99_latency}ms", file=sys.stderr)
          print(f"  CPU: {cpu_usage}%", file=sys.stderr)
          print(f"  Deployment: {deployment_id}", file=sys.stderr)
          print(f"  Health: {health_status}", file=sys.stderr)
          
      except Exception as e:
          print(f" ERROR parsing metrics: {e}", file=sys.stderr)
          import traceback
          traceback.print_exc(file=sys.stderr)
          
          Kestra.outputs({
              'error_rate': 0,
              'p95_latency': 0,
              'p99_latency': 0,
              'median_latency': 0,
              'rps': 0,
              'cpu_usage': 0,
              'memory_usage': 0,
              'deployment_id': 0,
              'health_status': 'unknown'
          })

  # PHASE 4 â€” ANOMALY DETECTION
  - id: anomaly_detection
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    beforeCommands:
      - pip install kestra
    script: |
      from kestra import Kestra
      import sys

      anomalies = []
      error_rate = {{ outputs.parse_metrics.vars.error_rate }}
      cpu = {{ outputs.parse_metrics.vars.cpu_usage }}
      memory = {{ outputs.parse_metrics.vars.memory_usage }}
      p99 = {{ outputs.parse_metrics.vars.p99_latency }}
      p95 = {{ outputs.parse_metrics.vars.p95_latency }}

      # Detect anomalies based on thresholds
      if error_rate > 5:
          anomalies.append(f"ERROR_RATE_SPIKE: {error_rate}%")
      if cpu > 85:
          anomalies.append(f"CPU_SPIKE: {cpu}%")
      if memory > 85:
          anomalies.append(f"MEMORY_SPIKE: {memory}%")
      if p99 > 500:
          anomalies.append(f"P99_LATENCY_SPIKE: {p99}ms")
      if p95 > 300:
          anomalies.append(f"P95_LATENCY_SPIKE: {p95}ms")

      anomaly_summary = ", ".join(anomalies) if anomalies else "No anomalies detected"

      Kestra.outputs({
          "anomalies": anomalies,
          "count": len(anomalies),
          "summary": anomaly_summary
      })

      print(f"   Anomaly Detection: {anomaly_summary}", file=sys.stderr)
      print(f"   Total anomalies: {len(anomalies)}", file=sys.stderr)

  # PHASE 5 â€” AI ANALYSIS
  - id: ai_incident_analysis
    type: io.kestra.plugin.openai.ChatCompletion
    description: AI decision engine with deployment history context
    apiKey: "{{ secret('OPENAI_API_KEY') }}"
    model: "gpt-4o-mini"
    temperature: 0.3
    messages:
      - role: system
        content: |
          You MUST respond with exactly this JSON structure:

          {
            "severity": "P0" | "P1" | "P2" | "P3",
            "confidence_score": 85,
            "incident_summary": "string",
            "root_cause_hypothesis": "string",
            "deployment_to_rollback": 100,
            "recommended_action": "rollback" | "monitor" | "escalate",
            "pagerduty_priority": "P1" | "P2" | "P3" | "P4" | "P5"
          }

          Rules:
          - confidence_score: 0-100, how confident you are in your analysis
          - Use "rollback" only if you're confident the deployment caused the issue
          - Use "monitor" for unclear situations
          - Use "escalate" for complex issues needing human intervention
          - Error rate > 10% is P0 critical
          - P99 latency > 800ms is P0 critical
          - Error rate > 5% or P99 > 500ms is P1
          - CPU/Memory > 90% is concerning
          - Consider deployment history when suggesting rollbacks
          - pagerduty_priority should align with severity (P0â†’P1, P1â†’P2, etc.)

          NO OTHER FIELDS. NO nested objects. NO markdown. ONLY this JSON object.

      - role: user
        content: |
          SERVICE: {{ inputs.service_name }}
          ENVIRONMENT: {{ inputs.environment }}
          TIMESTAMP: {{ execution.startDate }}

          DEPLOYMENT HISTORY:
          {{ outputs.consolidate_deployment_data.vars.deployment_history }}
          (Source: {{ outputs.consolidate_deployment_data.vars.source }})

          CURRENT METRICS:
          - Error Rate: {{ outputs.parse_metrics.vars.error_rate }}%
          - P95 Latency: {{ outputs.parse_metrics.vars.p95_latency }}ms
          - P99 Latency: {{ outputs.parse_metrics.vars.p99_latency }}ms
          - Median Latency: {{ outputs.parse_metrics.vars.median_latency }}ms
          - Requests/sec: {{ outputs.parse_metrics.vars.rps }}
          - CPU Usage: {{ outputs.parse_metrics.vars.cpu_usage }}%
          - Memory Usage: {{ outputs.parse_metrics.vars.memory_usage }}%
          - Current Deployment: {{ outputs.parse_metrics.vars.deployment_id }}
          - Health Status: {{ outputs.parse_metrics.vars.health_status }}

          DETECTED ANOMALIES ({{ outputs.anomaly_detection.vars.count }}):
          {{ outputs.anomaly_detection.vars.summary }}

          LOGS:
          {{ outputs.fetch_logs.body }}

          ALERTS:
          {{ outputs.fetch_alerts.body }}

          MIN_CONFIDENCE_THRESHOLD: {{ inputs.min_confidence_for_rollback }}

          Analyze the incident considering metrics, anomalies, and deployment history.
          If rollback is needed, suggest the previous deployment from the history.
          Respond with ONLY valid JSON.

  # PHASE 6 â€” PARSE AI OUTPUT
  - id: parse_ai_decision
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    beforeCommands:
      - pip install kestra
    script: |
      import json
      import sys
      from kestra import Kestra

      raw_content = """{{ outputs.ai_incident_analysis.choices[0].message.content }}"""

      content = raw_content.strip()

      # Remove markdown code blocks if present
      if content.startswith('```'):
          content = content[content.find('\n')+1:]
      if content.endswith('```'):
          content = content[:content.rfind('\n')]

      content = content.strip()

      try:
          data = json.loads(content)
      except json.JSONDecodeError as e:
          print(f"ERROR: Failed to parse JSON: {e}", file=sys.stderr)
          print(f"Raw content: {content}", file=sys.stderr)
          sys.exit(1)

      Kestra.outputs({
          'severity': data.get('severity', 'UNKNOWN'),
          'confidence_score': data.get('confidence_score', 0),
          'incident_summary': data.get('incident_summary', 'No summary'),
          'root_cause_hypothesis': data.get('root_cause_hypothesis', 'Unknown'),
          'deployment_to_rollback': data.get('deployment_to_rollback', 100),
          'recommended_action': data.get('recommended_action', 'monitor'),
          'pagerduty_priority': data.get('pagerduty_priority', 'P3')
      })

      print(json.dumps(data, indent=2), file=sys.stderr)

  # PHASE 7 â€” CREATE PAGERDUTY INCIDENT
  - id: create_pagerduty_incident
    type: io.kestra.plugin.core.flow.If
    condition: "{{ outputs.parse_ai_decision.vars.severity == 'P0' or outputs.parse_ai_decision.vars.severity == 'P1' }}"
    then:
      - id: check_pagerduty_token
        type: io.kestra.plugin.core.flow.If
        condition: "{{ inputs.pagerduty_token != 'mock' }}"
        then:
          - id: pagerduty_create_incident
            type: io.kestra.plugin.core.http.Request
            uri: "https://api.pagerduty.com/incidents"
            method: POST
            headers:
              Authorization: "Token token={{ inputs.pagerduty_token }}"
              Accept: "application/vnd.pagerduty+json;version=2"
              Content-Type: "application/json"
              From: "ai-incident-commander@kestra.io"
            body: |
              {
                "incident": {
                  "type": "incident",
                  "title": "[{{ outputs.parse_ai_decision.vars.severity }}] {{ inputs.service_name }} - {{ outputs.parse_ai_decision.vars.incident_summary }}",
                  "service": {
                    "id": "{{ inputs.pagerduty_service_id }}",
                    "type": "service_reference"
                  },
                  "urgency": "high",
                  "body": {
                    "type": "incident_body",
                    "details": "{{ outputs.parse_ai_decision.vars.root_cause_hypothesis }}\n\nAnomalies: {{ outputs.anomaly_detection.vars.summary }}\n\nError Rate: {{ outputs.parse_metrics.vars.error_rate }}%\nP99 Latency: {{ outputs.parse_metrics.vars.p99_latency }}ms\n\nExecution: {{ execution.id }}"
                  },
                  "escalation_policy": {
                    "id": "{{ inputs.pagerduty_escalation_policy }}",
                    "type": "escalation_policy_reference"
                  }
                }
              }
            allowFailed: true

          - id: log_pagerduty_success
            type: io.kestra.plugin.core.log.Log
            message: " PagerDuty incident created"
        else:
          - id: log_pagerduty_mock
            type: io.kestra.plugin.core.log.Log
            message: " PagerDuty mock mode - incident would be created with priority {{ outputs.parse_ai_decision.vars.pagerduty_priority }}"

  # PHASE 8 â€” CONFIDENCE CHECK & ESCALATION
  - id: check_confidence_threshold
    type: io.kestra.plugin.core.flow.If
    condition: "{{ outputs.parse_ai_decision.vars.confidence_score < 70 }}"
    then:
      - id: escalate_to_humans
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.slack_webhook }}"
        method: POST
        contentType: application/json
        body: |
          {
            "text": "<!channel> ðŸš¨ AI Confidence Low ({{ outputs.parse_ai_decision.vars.confidence_score }}%) - Human Review Required",
            "blocks": [
              {
                "type": "section",
                "text": {
                  "type": "mrkdwn",
                  "text": "*âš ï¸ LOW CONFIDENCE INCIDENT - HUMAN REVIEW NEEDED*\n\nThe AI is only {{ outputs.parse_ai_decision.vars.confidence_score }}% confident. Manual review required before taking action.\n\n*Incident Summary:*\n{{ outputs.parse_ai_decision.vars.incident_summary }}\n\n*Root Cause Hypothesis:*\n{{ outputs.parse_ai_decision.vars.root_cause_hypothesis }}\n\n*Detected Anomalies:*\n{{ outputs.anomaly_detection.vars.summary }}"
                }
              },
              {
                "type": "section",
                "fields": [
                  {
                    "type": "mrkdwn",
                    "text": "*Service:*\n{{ inputs.service_name }}"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*Deployment:*\n{{ outputs.parse_metrics.vars.deployment_id }}"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*Error Rate:*\n{{ outputs.parse_metrics.vars.error_rate }}%"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*P99 Latency:*\n{{ outputs.parse_metrics.vars.p99_latency }}ms"
                  }
                ]
              }
            ]
          }

  # PHASE 9 â€” ROUTE BY SEVERITY
  - id: route_by_severity
    type: io.kestra.plugin.core.flow.Switch
    value: "{{ outputs.parse_ai_decision.vars.severity }}"
    cases:
      P0:
        - id: handle_p0
          type: io.kestra.plugin.core.flow.Sequential
          tasks:
            - id: evaluate_rollback_decision
              type: io.kestra.plugin.core.flow.If
              condition: "{{ outputs.parse_ai_decision.vars.confidence_score >= inputs.min_confidence_for_rollback and outputs.parse_ai_decision.vars.recommended_action == 'rollback' }}"
              then:
                - id: perform_rollback
                  type: io.kestra.plugin.core.flow.Sequential
                  tasks:
                    # Trigger GitHub Action workflow_dispatch
                    - id: trigger_github_action
                      type: io.kestra.plugin.core.http.Request
                      uri: "https://api.github.com/repos/{{ inputs.deployments_repo }}/actions/workflows/rollback.yml/dispatches"
                      method: POST
                      headers:
                        Authorization: "token {{ inputs.github_token }}"
                        Accept: "application/vnd.github+json"
                        Content-Type: "application/json"
                      body: |
                        {
                          "ref": "master",
                          "inputs": {
                            "rollback_to": "{{ outputs.parse_ai_decision.vars.deployment_to_rollback }}",
                            "service": "{{ inputs.service_name }}",
                            "environment": "{{ inputs.environment }}",
                            "execution_id": "{{ execution.id }}"
                          }
                        }

                    - id: log_rollback_success
                      type: io.kestra.plugin.core.log.Log
                      message: "âœ” GitHub Action triggered for rollback â†’ {{ outputs.parse_ai_decision.vars.deployment_to_rollback }}"

              else:
                - id: skip_rollback_log
                  type: io.kestra.plugin.core.log.Log
                  message: "Rollback skipped - Confidence {{ outputs.parse_ai_decision.vars.confidence_score }}% or action '{{ outputs.parse_ai_decision.vars.recommended_action }}'"

            - id: slack_notify_p0
              type: io.kestra.plugin.core.http.Request
              uri: "{{ inputs.slack_webhook }}"
              method: POST
              contentType: application/json
              body: |
                {
                  "blocks": [
                    {
                      "type": "header",
                      "text": {
                        "type": "plain_text",
                        "text": "P0 CRITICAL INCIDENT",
                        "emoji": true
                      }
                    },
                    {
                      "type": "section",
                      "fields": [
                        {
                          "type": "mrkdwn",
                          "text": "*Service:*\n{{ inputs.service_name }}"
                        },
                        {
                          "type": "mrkdwn",
                          "text": "*Environment:*\n{{ inputs.environment }}"
                        },
                        {
                          "type": "mrkdwn",
                          "text": "*Severity:*\nP0"
                        },
                        {
                          "type": "mrkdwn",
                          "text": "*Confidence:*\n{{ outputs.parse_ai_decision.vars.confidence_score }}%"
                        }
                      ]
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Incident Summary:*\n{{ outputs.parse_ai_decision.vars.incident_summary }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Root Cause Hypothesis:*\n{{ outputs.parse_ai_decision.vars.root_cause_hypothesis }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Detected Anomalies:*\n{{ outputs.anomaly_detection.vars.summary }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Current Metrics:*\nâ€¢ Error Rate: {{ outputs.parse_metrics.vars.error_rate }}%\nâ€¢ P95 Latency: {{ outputs.parse_metrics.vars.p95_latency }}ms\nâ€¢ P99 Latency: {{ outputs.parse_metrics.vars.p99_latency }}ms\nâ€¢ CPU Usage: {{ outputs.parse_metrics.vars.cpu_usage }}%\nâ€¢ Deployment: {{ outputs.parse_metrics.vars.deployment_id }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Action Taken:*\n{{ outputs.parse_ai_decision.vars.recommended_action }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Deployment Source:*\n{{ outputs.consolidate_deployment_data.vars.source }}"
                      }
                    },
                    {
                      "type": "divider"
                    },
                    {
                      "type": "context",
                      "elements": [
                        {
                          "type": "mrkdwn",
                          "text": "Execution ID: {{ execution.id }} | Started: {{ execution.startDate }}"
                        }
                      ]
                    }
                  ]
                }

      P1:
        - id: handle_p1
          type: io.kestra.plugin.core.http.Request
          uri: "{{ inputs.slack_webhook }}"
          method: POST
          contentType: application/json
          body: |
            {
              "blocks": [
                {
                  "type": "header",
                  "text": {
                    "type": "plain_text",
                    "text": "P1 MAJOR INCIDENT",
                    "emoji": true
                  }
                },
                {
                  "type": "section",
                  "fields": [
                    {
                      "type": "mrkdwn",
                      "text": "*Service:*\n{{ inputs.service_name }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Confidence:*\n{{ outputs.parse_ai_decision.vars.confidence_score }}%"
                    }
                  ]
                },
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Summary:*\n{{ outputs.parse_ai_decision.vars.incident_summary }}"
                  }
                },
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Anomalies:*\n{{ outputs.anomaly_detection.vars.summary }}"
                  }
                },
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Metrics:*\nError Rate: {{ outputs.parse_metrics.vars.error_rate }}% | P99: {{ outputs.parse_metrics.vars.p99_latency }}ms"
                  }
                }
              ]
            }

      P2:
        - id: handle_p2
          type: io.kestra.plugin.core.log.Log
          message: "P2 incident logged: {{ outputs.parse_ai_decision.vars.incident_summary }}"

      P3:
        - id: handle_p3
          type: io.kestra.plugin.core.log.Log
          message: "P3 minor issue logged: {{ outputs.parse_ai_decision.vars.incident_summary }}"

      default:
        - id: handle_invalid_ai_output
          type: io.kestra.plugin.core.log.Log
          message: "Invalid AI output or missing severity."

  # PHASE 10 â€” HEALTH CHECK AFTER REMEDIATION
  - id: post_remediation_health_check
    type: io.kestra.plugin.core.flow.Sequential
    tasks:
      - id: wait_for_stabilization
        type: io.kestra.plugin.core.flow.Sleep
        duration: PT10S

      - id: fetch_post_metrics
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.metrics_url }}"
        method: GET

      - id: fetch_post_alerts
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.alerts_url }}"
        method: GET

      - id: parse_post_metrics
        type: io.kestra.plugin.scripts.python.Script
        docker:
          image: python:3.11-slim
        beforeCommands:
          - pip install kestra
        script: |
          import json
          import sys
          from kestra import Kestra

          try:
              raw_body = """{{ outputs.fetch_post_metrics.body }}"""
              
              if isinstance(raw_body, dict):
                  metrics = raw_body
              elif isinstance(raw_body, str):
                  metrics = json.loads(raw_body)
              else:
                  raise ValueError(f"Unexpected type: {type(raw_body)}")
              
              m = metrics.get('metrics', {})
              
              Kestra.outputs({
                  'error_rate': float(m.get('error_rate_percent', 0)),
                  'p99_latency': float(m.get('p99_latency_ms', 0)),
                  'deployment_id': int(metrics.get('deployment_id', 0)),
                  'health_status': str(metrics.get('health_status', 'unknown'))
              })
              
              print(f" Post-metrics parsed: deployment={metrics.get('deployment_id')}, error={m.get('error_rate_percent')}%", file=sys.stderr)
              
          except Exception as e:
              print(f" Error parsing post-metrics: {e}", file=sys.stderr)
              Kestra.outputs({
                  'error_rate': 0,
                  'p99_latency': 0,
                  'deployment_id': 0,
                  'health_status': 'unknown'
              })

      - id: compare_health_status
        type: io.kestra.plugin.scripts.python.Script
        docker:
          image: python:3.11-slim
        beforeCommands:
          - pip install kestra
        script: |
          import json
          import sys
          from kestra import Kestra

          before_error_rate = {{ outputs.parse_metrics.vars.error_rate }}
          after_error_rate = {{ outputs.parse_post_metrics.vars.error_rate }}

          before_p99 = {{ outputs.parse_metrics.vars.p99_latency }}
          after_p99 = {{ outputs.parse_post_metrics.vars.p99_latency }}

          before_deployment = {{ outputs.parse_metrics.vars.deployment_id }}
          after_deployment = {{ outputs.parse_post_metrics.vars.deployment_id }}

          if before_error_rate > 0:
              error_improvement_pct = ((before_error_rate - after_error_rate) / before_error_rate) * 100
          else:
              error_improvement_pct = 0

          if before_p99 > 0:
              latency_improvement_pct = ((before_p99 - after_p99) / before_p99) * 100
          else:
              latency_improvement_pct = 0

          if after_error_rate < 1 and after_p99 < 200:
              health_status = "FULLY_RECOVERED"
          elif after_error_rate < before_error_rate * 0.5 and latency_improvement_pct > 30:
              health_status = "SIGNIFICANT_IMPROVEMENT"
          elif after_error_rate < before_error_rate or after_p99 < before_p99:
              health_status = "IMPROVING"
          elif after_error_rate == before_error_rate and after_p99 == before_p99:
              health_status = "STABLE"
          else:
              health_status = "DEGRADED"

          Kestra.outputs({
              'health_status': health_status,
              'before_error_rate': before_error_rate,
              'after_error_rate': after_error_rate,
              'error_improvement_pct': round(error_improvement_pct, 2),
              'before_p99_latency': before_p99,
              'after_p99_latency': after_p99,
              'latency_improvement_pct': round(latency_improvement_pct, 2),
              'deployment_changed': before_deployment != after_deployment
          })

          print(f"Health check complete: {health_status}", file=sys.stderr)
          print(f"Error rate: {before_error_rate}% -> {after_error_rate}% ({error_improvement_pct:.2f}% improvement)", file=sys.stderr)
          print(f"P99 latency: {before_p99}ms -> {after_p99}ms ({latency_improvement_pct:.2f}% improvement)", file=sys.stderr)
          print(f"Deployment: {before_deployment} -> {after_deployment}", file=sys.stderr)

      - id: notify_health_status
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.slack_webhook }}"
        method: POST
        contentType: application/json
        body: |
          {
            "blocks": [
              {
                "type": "header",
                "text": {
                  "type": "plain_text",
                  "text": "Post-Remediation Health Check",
                  "emoji": true
                }
              },
              {
                "type": "section",
                "fields": [
                  {
                    "type": "mrkdwn",
                    "text": "*Status:*\n{{ outputs.compare_health_status.vars.health_status }}"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*Error Rate:*\n{{ outputs.compare_health_status.vars.before_error_rate }}% â†’ {{ outputs.compare_health_status.vars.after_error_rate }}%"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*P99 Latency:*\n{{ outputs.compare_health_status.vars.before_p99_latency }}ms â†’ {{ outputs.compare_health_status.vars.after_p99_latency }}ms"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*Improvement:*\nError: {{ outputs.compare_health_status.vars.error_improvement_pct }}%\nLatency: {{ outputs.compare_health_status.vars.latency_improvement_pct }}%"
                  }
                ]
              }
            ]
          }

  # PHASE 11 â€” SAVE INCIDENT SNAPSHOT
  - id: save_incident_snapshot
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    beforeCommands:
      - pip install kestra
    script: |
      import json
      from kestra import Kestra

      # Convert booleans safely
      chaos_mode = "{{ inputs.chaos_mode }}".lower() == "true"
      pagerduty_used = "{{ inputs.pagerduty_token }}" != "mock"
      deployment_changed = "{{ outputs.compare_health_status.vars.deployment_changed }}".lower() == "true"

      snapshot = {
          "execution_id": "{{ execution.id }}",
          "timestamp": "{{ execution.startDate }}",
          "service": "{{ inputs.service_name }}",
          "environment": "{{ inputs.environment }}",
          "chaos_mode": chaos_mode,
          "severity": "{{ outputs.parse_ai_decision.vars.severity }}",
          "confidence": float("{{ outputs.parse_ai_decision.vars.confidence_score }}"),
          "summary": "{{ outputs.parse_ai_decision.vars.incident_summary }}",
          "root_cause": "{{ outputs.parse_ai_decision.vars.root_cause_hypothesis }}",
          "action_taken": "{{ outputs.parse_ai_decision.vars.recommended_action }}",
          "deployment_source": "{{ outputs.consolidate_deployment_data.vars.source }}",
          "pagerduty_priority": "{{ outputs.parse_ai_decision.vars.pagerduty_priority }}",

          "anomalies": {
              "count": int("{{ outputs.anomaly_detection.vars.count }}"),
              "summary": "{{ outputs.anomaly_detection.vars.summary }}"
          },

          "metrics": {
              "before": {
                  "error_rate": float("{{ outputs.parse_metrics.vars.error_rate }}"),
                  "p99_latency": float("{{ outputs.parse_metrics.vars.p99_latency }}"),
                  "p95_latency": float("{{ outputs.parse_metrics.vars.p95_latency }}"),
                  "cpu_usage": float("{{ outputs.parse_metrics.vars.cpu_usage }}"),
                  "memory_usage": float("{{ outputs.parse_metrics.vars.memory_usage }}"),
                  "deployment_id": int("{{ outputs.parse_metrics.vars.deployment_id }}"),
                  "health_status": "{{ outputs.parse_metrics.vars.health_status }}"
              },
              "after": {
                  "error_rate": float("{{ outputs.compare_health_status.vars.after_error_rate }}"),
                  "p99_latency": float("{{ outputs.compare_health_status.vars.after_p99_latency }}"),
                  "deployment_id": int("{{ outputs.parse_post_metrics.vars.deployment_id }}"),
                  "health_status": "{{ outputs.parse_post_metrics.vars.health_status }}"
              },
              "improvement": {
                  "error_rate_pct": float("{{ outputs.compare_health_status.vars.error_improvement_pct }}"),
                  "latency_pct": float("{{ outputs.compare_health_status.vars.latency_improvement_pct }}")
              }
          },

          "health_status": "{{ outputs.compare_health_status.vars.health_status }}",
          "deployment_changed": deployment_changed,

          "integrations": {
              "github": "{{ outputs.consolidate_deployment_data.vars.source }}",
              "pagerduty": pagerduty_used,
              "slack": True
          }
      }

      Kestra.outputs(snapshot)

      print("=" * 60)
      print("INCIDENT SNAPSHOT SAVED")
      print(json.dumps(snapshot, indent=2))
      print("=" * 60)
