id: ai-incident-commander
namespace: hackathon.sre

description: |
  AI-powered autonomous incident response system that monitors, analyzes, and remediates production incidents.
  Features: Anomaly detection, chaos engineering mode, autonomous rollback, health monitoring, incident history.

inputs:
  - id: github_token
    type: STRING
    required: true
    defaults: "abc123"

  - id: pagerduty_token
    type: STRING
    required: true
    defaults: "abc123"

  - id: slack_webhook
    type: STRING
    required: true
    defaults: "https://hooks.slack.com/services/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"

  - id: deployments_repo
    type: STRING
    defaults: "demo/service"

  - id: k8s_namespace
    type: STRING
    defaults: "production"

  - id: service_name
    type: STRING
    defaults: "payment-api"

  - id: service_url
    type: STRING
    defaults: "http://payment-api:3000"

  - id: environment
    type: STRING
    defaults: "production"

  - id: metrics_url
    type: STRING
    defaults: "http://payment-api:3000/metrics/json"

  - id: logs_url
    type: STRING
    defaults: "http://payment-api:3000/logs"

  - id: alerts_url
    type: STRING
    defaults: "http://payment-api:3000/alerts"

  - id: min_confidence_for_rollback
    type: INT
    defaults: 80

  - id: enable_auto_remediation
    type: BOOLEAN
    defaults: true

  - id: chaos_mode
    type: BOOLEAN
    defaults: false
    description: "Enable chaos injection for testing (injects 600ms latency)"

tasks:
  # ============================================
  # PHASE 0 ‚Äî CHAOS INJECTION (OPTIONAL)
  # ============================================
  - id: chaos_inject
    type: io.kestra.plugin.core.flow.If
    condition: "{{ inputs.chaos_mode }}"
    then:
      - id: inject_latency
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.service_url }}/chaos/latency"
        method: POST
        contentType: application/json
        body: |
          {
            "latency_ms": 600,
            "duration_seconds": 30,
            "reason": "Chaos testing from execution {{ execution.id }}"
          }

      - id: log_chaos_injection
        type: io.kestra.plugin.core.log.Log
        message: "üî• Chaos mode: Injected 600ms latency for 30s"

  # ============================================
  # PHASE 1 ‚Äî OBSERVE
  # ============================================
  - id: collect_signals
    type: io.kestra.plugin.core.flow.Parallel
    tasks:
      - id: fetch_metrics
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.metrics_url }}"
        method: GET

      - id: fetch_logs
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.logs_url }}"
        method: GET

      - id: fetch_alerts
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.alerts_url }}"
        method: GET

  # ============================================
  # PHASE 2 ‚Äî PARSE METRICS
  # ============================================
  - id: parse_metrics
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    beforeCommands:
      - pip install kestra
    script: |
      import json
      import sys
      from kestra import Kestra

      try:
          # FIXED: Wrap template in triple quotes to handle string responses safely
          raw_body = """{{ outputs.fetch_metrics.body }}"""
          
          print(f"RAW BODY TYPE: {type(raw_body)}", file=sys.stderr)
          
          # Handle if it's already a dict or if it's a string
          if isinstance(raw_body, dict):
              metrics = raw_body
          elif isinstance(raw_body, str):
              metrics = json.loads(raw_body)
          else:
              raise ValueError(f"Unexpected type: {type(raw_body)}")
          
          print(f"PARSED METRICS: {json.dumps(metrics, indent=2)}", file=sys.stderr)
          
          # Extract metrics from JSON response
          m = metrics.get('metrics', {})
          
          error_rate = float(m.get('error_rate_percent', 0))
          p95_latency = float(m.get('p95_latency_ms', 0))
          p99_latency = float(m.get('p99_latency_ms', 0))
          median_latency = float(m.get('p50_latency_ms', 0))
          rps = float(m.get('requests_per_second', 0))
          cpu_usage = float(m.get('cpu_usage_percent', 0))
          memory_usage = float(m.get('memory_usage_percent', 0))
          deployment_id = int(metrics.get('deployment_id', 0))
          health_status = str(metrics.get('health_status', 'unknown'))
          
          Kestra.outputs({
              'error_rate': error_rate,
              'p95_latency': p95_latency,
              'p99_latency': p99_latency,
              'median_latency': median_latency,
              'rps': rps,
              'cpu_usage': cpu_usage,
              'memory_usage': memory_usage,
              'deployment_id': deployment_id,
              'health_status': health_status
          })
          
          print(f"‚úÖ Metrics parsed successfully:", file=sys.stderr)
          print(f"  Error rate: {error_rate}%", file=sys.stderr)
          print(f"  P99 latency: {p99_latency}ms", file=sys.stderr)
          print(f"  CPU: {cpu_usage}%", file=sys.stderr)
          print(f"  Deployment: {deployment_id}", file=sys.stderr)
          print(f"  Health: {health_status}", file=sys.stderr)
          
      except Exception as e:
          print(f"‚ùå ERROR parsing metrics: {e}", file=sys.stderr)
          print(f"Exception type: {type(e).__name__}", file=sys.stderr)
          import traceback
          traceback.print_exc(file=sys.stderr)
          
          Kestra.outputs({
              'error_rate': 0,
              'p95_latency': 0,
              'p99_latency': 0,
              'median_latency': 0,
              'rps': 0,
              'cpu_usage': 0,
              'memory_usage': 0,
              'deployment_id': 0,
              'health_status': 'unknown'
          })

  # ============================================
  # PHASE 3 ‚Äî ANOMALY DETECTION
  # ============================================
  - id: anomaly_detection
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    beforeCommands:
      - pip install kestra
    script: |
      from kestra import Kestra
      import sys

      anomalies = []
      error_rate = {{ outputs.parse_metrics.vars.error_rate }}
      cpu = {{ outputs.parse_metrics.vars.cpu_usage }}
      memory = {{ outputs.parse_metrics.vars.memory_usage }}
      p99 = {{ outputs.parse_metrics.vars.p99_latency }}
      p95 = {{ outputs.parse_metrics.vars.p95_latency }}

      # Detect anomalies based on thresholds
      if error_rate > 5:
          anomalies.append(f"ERROR_RATE_SPIKE: {error_rate}%")
      if cpu > 85:
          anomalies.append(f"CPU_SPIKE: {cpu}%")
      if memory > 85:
          anomalies.append(f"MEMORY_SPIKE: {memory}%")
      if p99 > 500:
          anomalies.append(f"P99_LATENCY_SPIKE: {p99}ms")
      if p95 > 300:
          anomalies.append(f"P95_LATENCY_SPIKE: {p95}ms")

      anomaly_summary = ", ".join(anomalies) if anomalies else "No anomalies detected"

      Kestra.outputs({
          "anomalies": anomalies,
          "count": len(anomalies),
          "summary": anomaly_summary
      })

      print(f"üîç Anomaly Detection: {anomaly_summary}", file=sys.stderr)
      print(f"   Total anomalies: {len(anomalies)}", file=sys.stderr)

  # ============================================
  # PHASE 4 ‚Äî AI ANALYSIS
  # ============================================
  - id: ai_incident_analysis
    type: io.kestra.plugin.openai.ChatCompletion
    description: AI decision engine with anomaly context
    apiKey: "{{ secret('OPENAI_API_KEY') }}"
    model: "gpt-4o-mini"
    temperature: 0.3
    messages:
      - role: system
        content: |
          You MUST respond with exactly this JSON structure:

          {
            "severity": "P0" | "P1" | "P2" | "P3",
            "confidence_score": 85,
            "incident_summary": "string",
            "root_cause_hypothesis": "string",
            "deployment_to_rollback": 100,
            "recommended_action": "rollback" | "monitor" | "escalate"
          }

          Rules:
          - confidence_score: 0-100, how confident you are in your analysis
          - Use "rollback" only if you're confident the deployment caused the issue
          - Use "monitor" for unclear situations
          - Use "escalate" for complex issues needing human intervention
          - Error rate > 10% is P0 critical
          - P99 latency > 800ms is P0 critical
          - Error rate > 5% or P99 > 500ms is P1
          - CPU/Memory > 90% is concerning
          - Look for correlation between deployment_id and degraded metrics
          - Consider the detected anomalies in your analysis
          - For deployment_to_rollback: suggest the previous known good deployment (typically deployment_id - 1)

          NO OTHER FIELDS.
          NO nested objects.
          NO markdown.
          ONLY this JSON object.

      - role: user
        content: |
          SERVICE: {{ inputs.service_name }}
          ENVIRONMENT: {{ inputs.environment }}
          TIMESTAMP: {{ execution.startDate }}

          CURRENT METRICS:
          - Error Rate: {{ outputs.parse_metrics.vars.error_rate }}%
          - P95 Latency: {{ outputs.parse_metrics.vars.p95_latency }}ms
          - P99 Latency: {{ outputs.parse_metrics.vars.p99_latency }}ms
          - Median Latency: {{ outputs.parse_metrics.vars.median_latency }}ms
          - Requests/sec: {{ outputs.parse_metrics.vars.rps }}
          - CPU Usage: {{ outputs.parse_metrics.vars.cpu_usage }}%
          - Memory Usage: {{ outputs.parse_metrics.vars.memory_usage }}%
          - Current Deployment: {{ outputs.parse_metrics.vars.deployment_id }}
          - Health Status: {{ outputs.parse_metrics.vars.health_status }}

          DETECTED ANOMALIES ({{ outputs.anomaly_detection.vars.count }}):
          {{ outputs.anomaly_detection.vars.summary }}

          LOGS:
          {{ outputs.fetch_logs.body }}

          ALERTS:
          {{ outputs.fetch_alerts.body }}

          MIN_CONFIDENCE_THRESHOLD: {{ inputs.min_confidence_for_rollback }}

          Analyze the incident considering both metrics and detected anomalies.
          Pay special attention to error rate and latency metrics - they are real production data.
          If deployment {{ outputs.parse_metrics.vars.deployment_id }} is causing issues, suggest rolling back to deployment {{ outputs.parse_metrics.vars.deployment_id - 1 }}
          Respond with ONLY valid JSON.

  # ============================================
  # PHASE 5 ‚Äî PARSE AI OUTPUT
  # ============================================
  - id: parse_ai_decision
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    beforeCommands:
      - pip install kestra
    script: |
      import json
      import sys
      from kestra import Kestra

      raw_content = """{{ outputs.ai_incident_analysis.choices[0].message.content }}"""

      content = raw_content.strip()

      # Remove markdown code blocks if present
      if content.startswith('```'):
          content = content[content.find('\n')+1:]
      if content.endswith('```'):
          content = content[:content.rfind('\n')]

      content = content.strip()

      try:
          data = json.loads(content)
      except json.JSONDecodeError as e:
          print(f"ERROR: Failed to parse JSON: {e}", file=sys.stderr)
          print(f"Raw content: {content}", file=sys.stderr)
          sys.exit(1)

      # Output variables using Kestra.outputs()
      Kestra.outputs({
          'severity': data.get('severity', 'UNKNOWN'),
          'confidence_score': data.get('confidence_score', 0),
          'incident_summary': data.get('incident_summary', 'No summary'),
          'root_cause_hypothesis': data.get('root_cause_hypothesis', 'Unknown'),
          'deployment_to_rollback': data.get('deployment_to_rollback', 100),
          'recommended_action': data.get('recommended_action', 'monitor')
      })

      # Debug output
      print(json.dumps(data, indent=2), file=sys.stderr)

  - id: check_confidence_threshold
    type: io.kestra.plugin.core.flow.If
    condition: "{{ outputs.parse_ai_decision.vars.confidence_score < 70 }}"
    then:
      - id: escalate_to_humans
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.slack_webhook }}"
        method: POST
        contentType: application/json
        body: |
          {
            "text": "<!channel> üö® AI Confidence Low ({{ outputs.parse_ai_decision.vars.confidence_score }}%) - Human Review Required",
            "blocks": [
              {
                "type": "section",
                "text": {
                  "type": "mrkdwn",
                  "text": "*‚ö†Ô∏è LOW CONFIDENCE INCIDENT - HUMAN REVIEW NEEDED*\n\nThe AI is only {{ outputs.parse_ai_decision.vars.confidence_score }}% confident. Manual review required before taking action.\n\n*Incident Summary:*\n{{ outputs.parse_ai_decision.vars.incident_summary }}\n\n*Root Cause Hypothesis:*\n{{ outputs.parse_ai_decision.vars.root_cause_hypothesis }}\n\n*Detected Anomalies:*\n{{ outputs.anomaly_detection.vars.summary }}"
                }
              },
              {
                "type": "section",
                "fields": [
                  {
                    "type": "mrkdwn",
                    "text": "*Service:*\n{{ inputs.service_name }}"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*Deployment:*\n{{ outputs.parse_metrics.vars.deployment_id }}"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*Error Rate:*\n{{ outputs.parse_metrics.vars.error_rate }}%"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*P99 Latency:*\n{{ outputs.parse_metrics.vars.p99_latency }}ms"
                  }
                ]
              }
            ]
          }

  # ============================================
  # PHASE 6 ‚Äî ROUTE BY SEVERITY
  # ============================================
  - id: route_by_severity
    type: io.kestra.plugin.core.flow.Switch
    value: "{{ outputs.parse_ai_decision.vars.severity }}"
    cases:
      P0:
        - id: handle_p0
          type: io.kestra.plugin.core.flow.Sequential
          tasks:
            # Check confidence before rollback
            - id: evaluate_rollback_decision
              type: io.kestra.plugin.core.flow.If
              condition: "{{ outputs.parse_ai_decision.vars.confidence_score >= inputs.min_confidence_for_rollback and outputs.parse_ai_decision.vars.recommended_action == 'rollback' }}"
              then:
                - id: perform_rollback
                  type: io.kestra.plugin.core.flow.Sequential
                  tasks:
                    - id: rollback
                      type: io.kestra.plugin.core.http.Request
                      uri: "{{ inputs.service_url }}/deployment/{{ outputs.parse_ai_decision.vars.deployment_to_rollback }}/activate"
                      method: POST
                      contentType: application/json
                      body: |
                        {
                          "reason": "Automatic rollback from deployment {{ outputs.parse_metrics.vars.deployment_id }}",
                          "confidence": {{ outputs.parse_ai_decision.vars.confidence_score }},
                          "rollback_to": {{ outputs.parse_ai_decision.vars.deployment_to_rollback }},
                          "execution_id": "{{ execution.id }}"
                        }

                    - id: log_rollback_success
                      type: io.kestra.plugin.core.log.Log
                      message: "‚úÖ Rollback completed: deployment {{ outputs.parse_metrics.vars.deployment_id }} ‚Üí {{ outputs.parse_ai_decision.vars.deployment_to_rollback }}"

              else:
                - id: skip_rollback_log
                  type: io.kestra.plugin.core.log.Log
                  message: "‚ö†Ô∏è Rollback skipped - Confidence {{ outputs.parse_ai_decision.vars.confidence_score }}% below threshold {{ inputs.min_confidence_for_rollback }}% or action is '{{ outputs.parse_ai_decision.vars.recommended_action }}'"

            - id: slack_notify_p0
              type: io.kestra.plugin.core.http.Request
              uri: "{{ inputs.slack_webhook }}"
              method: POST
              contentType: application/json
              body: |
                {
                  "blocks": [
                    {
                      "type": "header",
                      "text": {
                        "type": "plain_text",
                        "text": "üö® P0 CRITICAL INCIDENT",
                        "emoji": true
                      }
                    },
                    {
                      "type": "section",
                      "fields": [
                        {
                          "type": "mrkdwn",
                          "text": "*Service:*\n{{ inputs.service_name }}"
                        },
                        {
                          "type": "mrkdwn",
                          "text": "*Environment:*\n{{ inputs.environment }}"
                        },
                        {
                          "type": "mrkdwn",
                          "text": "*Severity:*\nP0"
                        },
                        {
                          "type": "mrkdwn",
                          "text": "*Confidence:*\n{{ outputs.parse_ai_decision.vars.confidence_score }}%"
                        }
                      ]
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Incident Summary:*\n{{ outputs.parse_ai_decision.vars.incident_summary }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Root Cause Hypothesis:*\n{{ outputs.parse_ai_decision.vars.root_cause_hypothesis }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Detected Anomalies:*\n{{ outputs.anomaly_detection.vars.summary }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Current Metrics:*\n‚Ä¢ Error Rate: {{ outputs.parse_metrics.vars.error_rate }}%\n‚Ä¢ P95 Latency: {{ outputs.parse_metrics.vars.p95_latency }}ms\n‚Ä¢ P99 Latency: {{ outputs.parse_metrics.vars.p99_latency }}ms\n‚Ä¢ CPU Usage: {{ outputs.parse_metrics.vars.cpu_usage }}%\n‚Ä¢ Deployment: {{ outputs.parse_metrics.vars.deployment_id }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Action Taken:*\n{{ outputs.parse_ai_decision.vars.recommended_action }}"
                      }
                    },
                    {
                      "type": "divider"
                    },
                    {
                      "type": "context",
                      "elements": [
                        {
                          "type": "mrkdwn",
                          "text": "Execution ID: {{ execution.id }} | Started: {{ execution.startDate }}"
                        }
                      ]
                    }
                  ]
                }

      P1:
        - id: handle_p1
          type: io.kestra.plugin.core.http.Request
          uri: "{{ inputs.slack_webhook }}"
          method: POST
          contentType: application/json
          body: |
            {
              "blocks": [
                {
                  "type": "header",
                  "text": {
                    "type": "plain_text",
                    "text": "‚ö†Ô∏è P1 MAJOR INCIDENT",
                    "emoji": true
                  }
                },
                {
                  "type": "section",
                  "fields": [
                    {
                      "type": "mrkdwn",
                      "text": "*Service:*\n{{ inputs.service_name }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Confidence:*\n{{ outputs.parse_ai_decision.vars.confidence_score }}%"
                    }
                  ]
                },
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Summary:*\n{{ outputs.parse_ai_decision.vars.incident_summary }}"
                  }
                },
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Anomalies:*\n{{ outputs.anomaly_detection.vars.summary }}"
                  }
                },
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Metrics:*\nError Rate: {{ outputs.parse_metrics.vars.error_rate }}% | P99: {{ outputs.parse_metrics.vars.p99_latency }}ms"
                  }
                }
              ]
            }

      P2:
        - id: handle_p2
          type: io.kestra.plugin.core.log.Log
          message: "P2 incident logged: {{ outputs.parse_ai_decision.vars.incident_summary }}"

      P3:
        - id: handle_p3
          type: io.kestra.plugin.core.log.Log
          message: "P3 minor issue logged: {{ outputs.parse_ai_decision.vars.incident_summary }}"

      default:
        - id: handle_invalid_ai_output
          type: io.kestra.plugin.core.log.Log
          message: "Invalid AI output or missing severity."

  # ============================================
  # PHASE 7 ‚Äî HEALTH CHECK AFTER REMEDIATION
  # ============================================
  - id: post_remediation_health_check
    type: io.kestra.plugin.core.flow.Sequential
    tasks:
      - id: wait_for_stabilization
        type: io.kestra.plugin.core.flow.Sleep
        duration: PT10S

      - id: fetch_post_metrics
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.metrics_url }}"
        method: GET

      - id: fetch_post_alerts
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.alerts_url }}"
        method: GET

      - id: parse_post_metrics
        type: io.kestra.plugin.scripts.python.Script
        docker:
          image: python:3.11-slim
        beforeCommands:
          - pip install kestra
        script: |
          import json
          import sys
          from kestra import Kestra

          try:
              # FIXED: Wrap template in triple quotes
              raw_body = """{{ outputs.fetch_post_metrics.body }}"""
              
              if isinstance(raw_body, dict):
                  metrics = raw_body
              elif isinstance(raw_body, str):
                  metrics = json.loads(raw_body)
              else:
                  raise ValueError(f"Unexpected type: {type(raw_body)}")
              
              m = metrics.get('metrics', {})
              
              Kestra.outputs({
                  'error_rate': float(m.get('error_rate_percent', 0)),
                  'p99_latency': float(m.get('p99_latency_ms', 0)),
                  'deployment_id': int(metrics.get('deployment_id', 0)),
                  'health_status': str(metrics.get('health_status', 'unknown'))
              })
              
              print(f"‚úÖ Post-metrics parsed: deployment={metrics.get('deployment_id')}, error={m.get('error_rate_percent')}%", file=sys.stderr)
              
          except Exception as e:
              print(f"‚ùå Error parsing post-metrics: {e}", file=sys.stderr)
              Kestra.outputs({
                  'error_rate': 0,
                  'p99_latency': 0,
                  'deployment_id': 0,
                  'health_status': 'unknown'
              })

      - id: compare_health_status
        type: io.kestra.plugin.scripts.python.Script
        docker:
          image: python:3.11-slim
        beforeCommands:
          - pip install kestra
        script: |
          import json
          import sys
          from kestra import Kestra

          # Get before and after metrics
          before_error_rate = {{ outputs.parse_metrics.vars.error_rate }}
          after_error_rate = {{ outputs.parse_post_metrics.vars.error_rate }}

          before_p99 = {{ outputs.parse_metrics.vars.p99_latency }}
          after_p99 = {{ outputs.parse_post_metrics.vars.p99_latency }}

          before_deployment = {{ outputs.parse_metrics.vars.deployment_id }}
          after_deployment = {{ outputs.parse_post_metrics.vars.deployment_id }}

          # Calculate improvements
          if before_error_rate > 0:
              error_improvement_pct = ((before_error_rate - after_error_rate) / before_error_rate) * 100
          else:
              error_improvement_pct = 0

          if before_p99 > 0:
              latency_improvement_pct = ((before_p99 - after_p99) / before_p99) * 100
          else:
              latency_improvement_pct = 0

          # Determine health status
          if after_error_rate < 1 and after_p99 < 200:
              health_status = "FULLY_RECOVERED"
          elif after_error_rate < before_error_rate * 0.5 and latency_improvement_pct > 30:
              health_status = "SIGNIFICANT_IMPROVEMENT"
          elif after_error_rate < before_error_rate or after_p99 < before_p99:
              health_status = "IMPROVING"
          elif after_error_rate == before_error_rate and after_p99 == before_p99:
              health_status = "STABLE"
          else:
              health_status = "DEGRADED"

          Kestra.outputs({
              'health_status': health_status,
              'before_error_rate': before_error_rate,
              'after_error_rate': after_error_rate,
              'error_improvement_pct': round(error_improvement_pct, 2),
              'before_p99_latency': before_p99,
              'after_p99_latency': after_p99,
              'latency_improvement_pct': round(latency_improvement_pct, 2),
              'deployment_changed': before_deployment != after_deployment
          })

          print(f"Health check complete: {health_status}", file=sys.stderr)
          print(f"Error rate: {before_error_rate}% -> {after_error_rate}% ({error_improvement_pct:.2f}% improvement)", file=sys.stderr)
          print(f"P99 latency: {before_p99}ms -> {after_p99}ms ({latency_improvement_pct:.2f}% improvement)", file=sys.stderr)
          print(f"Deployment: {before_deployment} -> {after_deployment}", file=sys.stderr)

      - id: notify_health_status
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.slack_webhook }}"
        method: POST
        contentType: application/json
        body: |
          {
            "blocks": [
              {
                "type": "header",
                "text": {
                  "type": "plain_text",
                  "text": "üè• Post-Remediation Health Check",
                  "emoji": true
                }
              },
              {
                "type": "section",
                "fields": [
                  {
                    "type": "mrkdwn",
                    "text": "*Status:*\n{{ outputs.compare_health_status.vars.health_status }}"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*Error Rate:*\n{{ outputs.compare_health_status.vars.before_error_rate }}% ‚Üí {{ outputs.compare_health_status.vars.after_error_rate }}%"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*P99 Latency:*\n{{ outputs.compare_health_status.vars.before_p99_latency }}ms ‚Üí {{ outputs.compare_health_status.vars.after_p99_latency }}ms"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*Improvement:*\nError: {{ outputs.compare_health_status.vars.error_improvement_pct }}%\nLatency: {{ outputs.compare_health_status.vars.latency_improvement_pct }}%"
                  }
                ]
              }
            ]
          }

  # ============================================
  # PHASE 8 ‚Äî SAVE INCIDENT SNAPSHOT
  # ============================================
  - id: save_incident_snapshot
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    beforeCommands:
      - pip install kestra
    script: |
      import json
      from kestra import Kestra

      snapshot = {
          "execution_id": "{{ execution.id }}",
          "timestamp": "{{ execution.startDate }}",
          "service": "{{ inputs.service_name }}",
          "environment": "{{ inputs.environment }}",
          "chaos_mode": "{{ inputs.chaos_mode }}",
          "severity": "{{ outputs.parse_ai_decision.vars.severity }}",
          "confidence": {{ outputs.parse_ai_decision.vars.confidence_score }},
          "summary": "{{ outputs.parse_ai_decision.vars.incident_summary }}",
          "root_cause": "{{ outputs.parse_ai_decision.vars.root_cause_hypothesis }}",
          "action_taken": "{{ outputs.parse_ai_decision.vars.recommended_action }}",
          "anomalies": {
              "count": {{ outputs.anomaly_detection.vars.count }},
              "summary": "{{ outputs.anomaly_detection.vars.summary }}"
          },
          "metrics": {
              "before": {
                  "error_rate": {{ outputs.parse_metrics.vars.error_rate }},
                  "p99_latency": {{ outputs.parse_metrics.vars.p99_latency }},
                  "p95_latency": {{ outputs.parse_metrics.vars.p95_latency }},
                  "cpu_usage": {{ outputs.parse_metrics.vars.cpu_usage }},
                  "memory_usage": {{ outputs.parse_metrics.vars.memory_usage }},
                  "deployment_id": {{ outputs.parse_metrics.vars.deployment_id }},
                  "health_status": "{{ outputs.parse_metrics.vars.health_status }}"
              },
              "after": {
                  "error_rate": {{ outputs.compare_health_status.vars.after_error_rate }},
                  "p99_latency": {{ outputs.compare_health_status.vars.after_p99_latency }},
                  "deployment_id": {{ outputs.parse_post_metrics.vars.deployment_id }},
                  "health_status": "{{ outputs.parse_post_metrics.vars.health_status }}"
              },
              "improvement": {
                  "error_rate_pct": {{ outputs.compare_health_status.vars.error_improvement_pct }},
                  "latency_pct": {{ outputs.compare_health_status.vars.latency_improvement_pct }}
              }
          },
          "health_status": "{{ outputs.compare_health_status.vars.health_status }}",
          "deployment_changed": "{{ outputs.compare_health_status.vars.deployment_changed }}",
      }

      Kestra.outputs(snapshot)

      print("=" * 60)
      print("üìä INCIDENT SNAPSHOT SAVED")
      print("=" * 60)
      print(json.dumps(snapshot, indent=2))
      print("=" * 60)
