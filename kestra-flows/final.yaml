id: ai-incident-commander
namespace: hackathon.sre

description: |
  AI-powered autonomous incident response system that monitors, analyzes, and remediates production incidents.
  Features: Anomaly detection, chaos engineering mode, autonomous rollback, health monitoring, incident history.
  Integrations: GitHub (deployment tracking), PagerDuty (incident management), Slack (notifications).

inputs:
  - id: github_token
    type: STRING
    required: true
    defaults: "mock"
    description: "GitHub token for deployment history. Use 'mock' for demo mode."

  - id: pagerduty_token
    type: STRING
    required: true
    defaults: "mock"
    description: "PagerDuty API token. Use 'mock' for demo mode."

  - id: slack_webhook
    type: STRING
    required: true
    defaults: "https://hooks.slack.com/services/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"
    description: "Enter your own slack webhook here to recieve messages."

  - id: deployments_repo
    type: STRING
    defaults: "demo/service"
    description: "GitHub repo in format 'owner/repo'"

  - id: service_name
    type: STRING
    defaults: "payment-api"

  - id: service_url
    type: STRING
    defaults: "http://payment-api:3000"

  - id: environment
    type: STRING
    defaults: "production"

  - id: metrics_url
    type: STRING
    defaults: "http://payment-api:3000/metrics/json"

  - id: logs_url
    type: STRING
    defaults: "http://payment-api:3000/logs"

  - id: alerts_url
    type: STRING
    defaults: "http://payment-api:3000/alerts"

  - id: min_confidence_for_rollback
    type: INT
    defaults: 80

  - id: chaos_mode
    type: BOOLEAN
    defaults: false
    description: "Enable chaos injection for testing (injects 600ms latency)"

  - id: pagerduty_service_id
    type: STRING
    defaults: "PSERVICE1"
    description: "PagerDuty service ID for incident creation"

  - id: pagerduty_escalation_policy
    type: STRING
    defaults: "PEPOLICY1"
    description: "PagerDuty escalation policy ID"

  - id: chaos_type
    type: STRING
    defaults: "latency"
    description: "Type of chaos: latency, cpu-stress, memory-stress, network-delay, pod-kill"

  - id: chaos_duration
    type: STRING
    defaults: "30s"
    description: "Duration for chaos experiment (e.g., 30s, 1m, 2m)"

  - id: chaos_intensity
    type: STRING
    defaults: "medium"
    description: "Intensity: low, medium, high"

tasks:
  # PHASE 0 â€” CHAOS INJECTION (OPTIONAL)
  - id: chaos_inject
    type: io.kestra.plugin.core.flow.If
    condition: "{{ inputs.chaos_mode }}"
    then:
      - id: determine_chaos_config
        type: io.kestra.plugin.scripts.python.Script
        docker:
          image: python:3.11-slim
        beforeCommands:
          - pip install kestra
        script: |
          import json
          import sys
          from kestra import Kestra

          chaos_type = "{{ inputs.chaos_type }}"
          intensity = "{{ inputs.chaos_intensity }}"
          duration = {{ inputs.chaos_duration }}

          # Define chaos configurations based on type and intensity
          configs = {
              "latency": {
                  "low": {"latency_ms": 200, "endpoint": "/chaos/latency"},
                  "medium": {"latency_ms": 600, "endpoint": "/chaos/latency"},
                  "high": {"latency_ms": 1500, "endpoint": "/chaos/latency"}
              },
              "error-rate": {
                  "low": {"error_rate_percent": 5, "endpoint": "/chaos/errors"},
                  "medium": {"error_rate_percent": 15, "endpoint": "/chaos/errors"},
                  "high": {"error_rate_percent": 30, "endpoint": "/chaos/errors"}
              },
              "cpu-spike": {
                  "low": {"cpu_percent": 70, "endpoint": "/chaos/cpu"},
                  "medium": {"cpu_percent": 85, "endpoint": "/chaos/cpu"},
                  "high": {"cpu_percent": 95, "endpoint": "/chaos/cpu"}
              }
          }

          config = configs.get(chaos_type, {}).get(intensity, configs["latency"]["medium"])
          config["chaos_type"] = chaos_type
          config["duration"] = duration

          Kestra.outputs({
              "config": json.dumps(config),
              "chaos_type": chaos_type,
              "endpoint": config.get("endpoint", "/chaos/latency"),
              "latency_ms": config.get("latency_ms", 0),
              "error_rate_percent": config.get("error_rate_percent", 0),
              "cpu_percent": config.get("cpu_percent", 0),
              "duration_seconds": duration
          })

          print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—", file=sys.stderr)
          print("â•‘    ðŸŒªï¸  CHAOS INJECTION ACTIVE  ðŸŒªï¸     â•‘", file=sys.stderr)
          print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£", file=sys.stderr)
          print(f"â•‘  Type:      {chaos_type:24s} â•‘", file=sys.stderr)
          print(f"â•‘  Intensity: {intensity:24s} â•‘", file=sys.stderr)
          print(f"â•‘  Duration:  {duration}s{' ' * (23-len(str(duration)))} â•‘", file=sys.stderr)
          print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•", file=sys.stderr)

      - id: inject_http_chaos
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.service_url }}{{ outputs.determine_chaos_config.vars.endpoint }}"
        method: POST
        contentType: application/json
        body: |
          {
            "latency_ms": {{ outputs.determine_chaos_config.vars.latency_ms }},
            "error_rate_percent": {{ outputs.determine_chaos_config.vars.error_rate_percent }},
            "cpu_percent": {{ outputs.determine_chaos_config.vars.cpu_percent }},
            "duration_seconds": {{ outputs.determine_chaos_config.vars.duration_seconds }},
            "reason": "Chaos testing from execution {{ execution.id }}"
          }
        allowFailed: true

      - id: verify_chaos_injection
        type: io.kestra.plugin.scripts.python.Script
        docker:
          image: python:3.11-slim
        script: |
          import sys

          status_code = {{ outputs.inject_http_chaos.code }}

          if status_code == 200:
              print("âœ… Chaos successfully injected into {{ inputs.service_name }}", file=sys.stderr)
              print("   Type: {{ outputs.determine_chaos_config.vars.chaos_type }}", file=sys.stderr)
              print("   Duration: {{ outputs.determine_chaos_config.vars.duration_seconds }}s", file=sys.stderr)
          else:
              print("âš ï¸  Chaos injection returned status {{ outputs.inject_http_chaos.code }}", file=sys.stderr)
              print("   This is OK for demo mode - continuing with workflow", file=sys.stderr)

      - id: wait_for_chaos_to_propagate
        type: io.kestra.plugin.core.flow.Sleep
        duration: PT5S

  # PHASE 1 â€” FETCH DEPLOYMENT HISTORY (GitHub)
  - id: fetch_deployment_history
    type: io.kestra.plugin.core.flow.If
    condition: "{{ inputs.github_token != 'mock' }}"
    then:
      - id: github_fetch_deployments
        type: io.kestra.plugin.core.http.Request
        uri: "https://api.github.com/repos/{{ inputs.deployments_repo }}/deployments"
        method: GET
        headers:
          Authorization: "token {{ inputs.github_token }}"
          Accept: "application/vnd.github.v3+json"
        allowFailed: true

      - id: parse_github_deployments
        type: io.kestra.plugin.scripts.python.Script
        docker:
          image: python:3.11-slim
        beforeCommands:
          - pip install kestra
        script: |
          import json
          import sys
          from kestra import Kestra

          try:
              deployments_raw = """{{ outputs.github_fetch_deployments.body }}"""
              deployments = json.loads(deployments_raw) if isinstance(deployments_raw, str) else deployments_raw
              
              # Extract last 5 deployments
              deployment_history = []
              for dep in deployments[:5]:
                  deployment_history.append({
                      "id": dep.get("id"),
                      "sha": dep.get("sha", "unknown")[:7],
                      "environment": dep.get("environment", "unknown"),
                      "created_at": dep.get("created_at", "unknown"),
                      "ref": dep.get("ref", "unknown")
                  })
              
              Kestra.outputs({
                  "deployment_history": json.dumps(deployment_history),
                  "latest_sha": deployment_history[0]["sha"] if deployment_history else "unknown",
                  "source": "github"
              })
              
              print(f"Fetched {len(deployment_history)} deployments from GitHub", file=sys.stderr)
              
          except Exception as e:
              print(f"GitHub fetch failed: {e}, using mock data", file=sys.stderr)
              mock_history = [
                  {"id": 101, "sha": "abc1234", "environment": "production", "created_at": "2025-12-12T10:00:00Z", "ref": "main"},
                  {"id": 100, "sha": "def5678", "environment": "production", "created_at": "2025-12-11T15:30:00Z", "ref": "main"}
              ]
              Kestra.outputs({
                  "deployment_history": json.dumps(mock_history),
                  "latest_sha": "abc1234",
                  "source": "mock"
              })
    else:
      - id: use_mock_deployments
        type: io.kestra.plugin.scripts.python.Script
        docker:
          image: python:3.11-slim
        beforeCommands:
          - pip install kestra
        script: |
          import json
          import sys
          from kestra import Kestra

          mock_history = [
              {"id": 101, "sha": "abc1234", "environment": "production", "created_at": "2025-12-12T10:00:00Z", "ref": "main"},
              {"id": 100, "sha": "def5678", "environment": "production", "created_at": "2025-12-11T15:30:00Z", "ref": "main"},
              {"id": 99, "sha": "ghi9012", "environment": "production", "created_at": "2025-12-10T09:15:00Z", "ref": "main"}
          ]

          Kestra.outputs({
              "deployment_history": json.dumps(mock_history),
              "latest_sha": "abc1234",
              "source": "mock"
          })

          print("ðŸ“¦ Using mock deployment history", file=sys.stderr)

  # PHASE 1B â€” CONSOLIDATE DEPLOYMENT DATA
  - id: consolidate_deployment_data
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    beforeCommands:
      - pip install kestra
    script: |
      import json
      import sys
      from kestra import Kestra

      # Read GitHub outputs safely (if they exist)
      gh_history = """{{ outputs.parse_github_deployments.deployment_history | default('') }}"""
      gh_source = """{{ outputs.parse_github_deployments.source | default('') }}"""

      if gh_history.strip():
          # GitHub returned real data
          Kestra.outputs({
              "deployment_history": gh_history,
              "source": gh_source or "github"
          })
          print("Using GitHub deployment history", file=sys.stderr)
      else:
          # Use mock fallback (task always exists when token == 'mock')
          mock_history = """{{ outputs.use_mock_deployments.deployment_history | default('') }}"""
          mock_source = """{{ outputs.use_mock_deployments.source | default('mock') }}"""

          Kestra.outputs({
              "deployment_history": mock_history,
              "source": mock_source
          })
          print("Using MOCK deployment history", file=sys.stderr)

  # PHASE 2 â€” OBSERVE
  - id: collect_signals
    type: io.kestra.plugin.core.flow.Parallel
    tasks:
      - id: fetch_metrics
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.metrics_url }}"
        method: GET

      - id: fetch_logs
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.logs_url }}"
        method: GET

      - id: fetch_alerts
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.alerts_url }}"
        method: GET

  # PHASE 3 â€” PARSE METRICS
  - id: parse_metrics
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    beforeCommands:
      - pip install kestra
    script: |
      import json
      import sys
      from kestra import Kestra

      try:
          raw_body = """{{ outputs.fetch_metrics.body }}"""
          
          if isinstance(raw_body, dict):
              metrics = raw_body
          elif isinstance(raw_body, str):
              metrics = json.loads(raw_body)
          else:
              raise ValueError(f"Unexpected type: {type(raw_body)}")
          
          m = metrics.get('metrics', {})
          
          error_rate = float(m.get('error_rate_percent', 0))
          p95_latency = float(m.get('p95_latency_ms', 0))
          p99_latency = float(m.get('p99_latency_ms', 0))
          median_latency = float(m.get('p50_latency_ms', 0))
          rps = float(m.get('requests_per_second', 0))
          cpu_usage = float(m.get('cpu_usage_percent', 0))
          memory_usage = float(m.get('memory_usage_percent', 0))
          deployment_id = int(metrics.get('deployment_id', 0))
          health_status = str(metrics.get('health_status', 'unknown'))
          
          Kestra.outputs({
              'error_rate': error_rate,
              'p95_latency': p95_latency,
              'p99_latency': p99_latency,
              'median_latency': median_latency,
              'rps': rps,
              'cpu_usage': cpu_usage,
              'memory_usage': memory_usage,
              'deployment_id': deployment_id,
              'health_status': health_status
          })
          
          print(f"  Metrics parsed successfully:", file=sys.stderr)
          print(f"  Error rate: {error_rate}%", file=sys.stderr)
          print(f"  P99 latency: {p99_latency}ms", file=sys.stderr)
          print(f"  CPU: {cpu_usage}%", file=sys.stderr)
          print(f"  Deployment: {deployment_id}", file=sys.stderr)
          print(f"  Health: {health_status}", file=sys.stderr)
          
      except Exception as e:
          print(f" ERROR parsing metrics: {e}", file=sys.stderr)
          import traceback
          traceback.print_exc(file=sys.stderr)
          
          Kestra.outputs({
              'error_rate': 0,
              'p95_latency': 0,
              'p99_latency': 0,
              'median_latency': 0,
              'rps': 0,
              'cpu_usage': 0,
              'memory_usage': 0,
              'deployment_id': 0,
              'health_status': 'unknown'
          })

  # PHASE 4 â€” ANOMALY DETECTION (IMPROVED)
  - id: anomaly_detection
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    beforeCommands:
      - pip install kestra
    script: |
      from kestra import Kestra
      import sys

      anomalies = []
      error_rate = {{ outputs.parse_metrics.vars.error_rate }}
      cpu = {{ outputs.parse_metrics.vars.cpu_usage }}
      memory = {{ outputs.parse_metrics.vars.memory_usage }}
      p99 = {{ outputs.parse_metrics.vars.p99_latency }}
      p95 = {{ outputs.parse_metrics.vars.p95_latency }}

      # CRITICAL anomalies (P0 triggers)
      critical_anomalies = []
      if error_rate > 10:
          critical_anomalies.append(f"CRITICAL_ERROR_RATE: {error_rate}%")
      if p99 > 800:
          critical_anomalies.append(f"CRITICAL_P99_LATENCY: {p99}ms")
      if cpu > 90:
          critical_anomalies.append(f"CRITICAL_CPU: {cpu}%")
      if memory > 90:
          critical_anomalies.append(f"CRITICAL_MEMORY: {memory}%")

      # WARNING anomalies (P1 triggers)
      warning_anomalies = []
      if 5 < error_rate <= 10:
          warning_anomalies.append(f"WARNING_ERROR_RATE: {error_rate}%")
      if 500 < p99 <= 800:
          warning_anomalies.append(f"WARNING_P99_LATENCY: {p99}ms")
      if 85 < cpu <= 90:
          warning_anomalies.append(f"WARNING_CPU: {cpu}%")
      if 85 < memory <= 90:
          warning_anomalies.append(f"WARNING_MEMORY: {memory}%")

      # MINOR anomalies (P2 triggers)
      minor_anomalies = []
      if 2 < error_rate <= 5:
          minor_anomalies.append(f"MINOR_ERROR_RATE: {error_rate}%")
      if 300 < p99 <= 500:
          minor_anomalies.append(f"MINOR_P99_LATENCY: {p99}ms")
      if 200 < p95 <= 400:
          minor_anomalies.append(f"MINOR_P95_LATENCY: {p95}ms")
      if 70 < cpu <= 85:
          minor_anomalies.append(f"MINOR_CPU: {cpu}%")
      if 70 < memory <= 85:
          minor_anomalies.append(f"MINOR_MEMORY: {memory}%")

      # Combine all anomalies
      all_anomalies = critical_anomalies + warning_anomalies + minor_anomalies

      # Create summary
      if critical_anomalies:
          severity_hint = "CRITICAL"
          summary = f"CRITICAL: {', '.join(critical_anomalies)}"
      elif warning_anomalies:
          severity_hint = "WARNING"
          summary = f"WARNING: {', '.join(warning_anomalies)}"
      elif minor_anomalies:
          severity_hint = "MINOR"
          summary = f"MINOR: {', '.join(minor_anomalies)}"
      else:
          severity_hint = "NORMAL"
          summary = "No anomalies detected"

      # Add context if there are other anomalies
      if len(all_anomalies) > 1:
          summary += f" (Total: {len(all_anomalies)} anomalies)"

      Kestra.outputs({
          "anomalies": all_anomalies,
          "critical_count": len(critical_anomalies),
          "warning_count": len(warning_anomalies),
          "minor_count": len(minor_anomalies),
          "count": len(all_anomalies),
          "summary": summary,
          "severity_hint": severity_hint
      })

      print(f"ðŸ” Anomaly Detection Summary:", file=sys.stderr)
      print(f"   Severity Hint: {severity_hint}", file=sys.stderr)
      print(f"   Critical: {len(critical_anomalies)}", file=sys.stderr)
      print(f"   Warning: {len(warning_anomalies)}", file=sys.stderr)
      print(f"   Minor: {len(minor_anomalies)}", file=sys.stderr)
      print(f"   Summary: {summary}", file=sys.stderr)

  # PHASE 5 â€” AI ANALYSIS (IMPROVED)
  - id: ai_incident_analysis
    type: io.kestra.plugin.openai.ChatCompletion
    description: AI decision engine with deployment history context
    apiKey: "{{ secret('OPENAI_API_KEY') }}"
    model: "gpt-4o-mini"
    temperature: 0.3
    messages:
      - role: system
        content: |
          You MUST respond with exactly this JSON structure:

          {
            "severity": "P0" | "P1" | "P2" | "P3",
            "confidence_score": 85,
            "incident_summary": "string",
            "root_cause_hypothesis": "string",
            "deployment_to_rollback": 100,
            "recommended_action": "rollback" | "monitor" | "escalate",
            "pagerduty_priority": "P1" | "P2" | "P3" | "P4" | "P5"
          }

          STRICT SEVERITY RULES (follow exactly):

          P0 (CRITICAL) - Use ONLY when:
          - Error rate > 10% OR
          - P99 latency > 800ms OR
          - CPU > 90% OR
          - Memory > 90% OR
          - ANY anomaly starts with "CRITICAL_"

          P1 (MAJOR) - Use when:
          - 5% < Error rate â‰¤ 10% OR
          - 500ms < P99 latency â‰¤ 800ms OR
          - 85% < CPU â‰¤ 90% OR
          - 85% < Memory â‰¤ 90% OR
          - ANY anomaly starts with "WARNING_"
          - NO CRITICAL anomalies present

          P2 (MODERATE) - Use when:
          - 2% < Error rate â‰¤ 5% OR
          - 300ms < P99 latency â‰¤ 500ms OR
          - 70% < CPU â‰¤ 85% OR
          - ANY anomaly starts with "MINOR_"
          - NO WARNING or CRITICAL anomalies present

          P3 (NORMAL) - Use when:
          - Error rate â‰¤ 2% AND
          - P99 latency â‰¤ 300ms AND
          - CPU â‰¤ 70% AND
          - Memory â‰¤ 70% AND
          - No anomalies detected OR all metrics are in healthy range

          Additional Rules:
          - confidence_score: 0-100, how confident you are in your analysis
          - Use "rollback" only if you're confident a recent deployment caused the issue
          - Use "monitor" for unclear situations or when metrics are borderline
          - Use "escalate" for complex issues needing human intervention
          - Consider deployment history when suggesting rollbacks
          - If anomaly_severity_hint is provided, strongly consider it
          - pagerduty_priority should align with severity (P0â†’P1, P1â†’P2, etc.)

          NO OTHER FIELDS. NO nested objects. NO markdown. ONLY this JSON object.

      - role: user
        content: |
          SERVICE: {{ inputs.service_name }}
          ENVIRONMENT: {{ inputs.environment }}
          TIMESTAMP: {{ execution.startDate }}

          DEPLOYMENT HISTORY:
          {{ outputs.consolidate_deployment_data.vars.deployment_history }}
          (Source: {{ outputs.consolidate_deployment_data.vars.source }})

          CURRENT METRICS:
          - Error Rate: {{ outputs.parse_metrics.vars.error_rate }}%
          - P95 Latency: {{ outputs.parse_metrics.vars.p95_latency }}ms
          - P99 Latency: {{ outputs.parse_metrics.vars.p99_latency }}ms
          - Median Latency: {{ outputs.parse_metrics.vars.median_latency }}ms
          - Requests/sec: {{ outputs.parse_metrics.vars.rps }}
          - CPU Usage: {{ outputs.parse_metrics.vars.cpu_usage }}%
          - Memory Usage: {{ outputs.parse_metrics.vars.memory_usage }}%
          - Current Deployment: {{ outputs.parse_metrics.vars.deployment_id }}
          - Health Status: {{ outputs.parse_metrics.vars.health_status }}

          ANOMALY ANALYSIS:
          - Total Anomalies: {{ outputs.anomaly_detection.vars.count }}
          - Critical Anomalies: {{ outputs.anomaly_detection.vars.critical_count }}
          - Warning Anomalies: {{ outputs.anomaly_detection.vars.warning_count }}
          - Minor Anomalies: {{ outputs.anomaly_detection.vars.minor_count }}
          - Severity Hint: {{ outputs.anomaly_detection.vars.severity_hint }}
          - Summary: {{ outputs.anomaly_detection.vars.summary }}

          LOGS:
          {{ outputs.fetch_logs.body }}

          ALERTS:
          {{ outputs.fetch_alerts.body }}

          MIN_CONFIDENCE_THRESHOLD: {{ inputs.min_confidence_for_rollback }}

          Analyze the incident following the STRICT SEVERITY RULES above.
          Pay special attention to the Severity Hint and anomaly counts.
          If rollback is needed, suggest the previous deployment from the history.
          Respond with ONLY valid JSON.

  # PHASE 6 â€” PARSE AI OUTPUT
  - id: parse_ai_decision
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    beforeCommands:
      - pip install kestra
    script: |
      import json
      import sys
      from kestra import Kestra

      raw_content = """{{ outputs.ai_incident_analysis.choices[0].message.content }}"""

      content = raw_content.strip()

      # Remove markdown code blocks if present
      if content.startswith('```'):
          content = content[content.find('\n')+1:]
      if content.endswith('```'):
          content = content[:content.rfind('\n')]

      content = content.strip()

      try:
          data = json.loads(content)
      except json.JSONDecodeError as e:
          print(f"ERROR: Failed to parse JSON: {e}", file=sys.stderr)
          print(f"Raw content: {content}", file=sys.stderr)
          sys.exit(1)

      Kestra.outputs({
          'severity': data.get('severity', 'UNKNOWN'),
          'confidence_score': data.get('confidence_score', 0),
          'incident_summary': data.get('incident_summary', 'No summary'),
          'root_cause_hypothesis': data.get('root_cause_hypothesis', 'Unknown'),
          'deployment_to_rollback': data.get('deployment_to_rollback', 100),
          'recommended_action': data.get('recommended_action', 'monitor'),
          'pagerduty_priority': data.get('pagerduty_priority', 'P3')
      })

      print(json.dumps(data, indent=2), file=sys.stderr)

  # PHASE 8 â€” CONFIDENCE CHECK & ESCALATION
  - id: check_confidence_threshold
    type: io.kestra.plugin.core.flow.If
    condition: "{{ outputs.parse_ai_decision.vars.confidence_score < 70 }}"
    then:
      - id: escalate_to_humans
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.slack_webhook }}"
        method: POST
        contentType: application/json
        body: |
          {
            "text": "<!channel> ðŸš¨ AI Confidence Low ({{ outputs.parse_ai_decision.vars.confidence_score }}%) - Human Review Required",
            "blocks": [
              {
                "type": "section",
                "text": {
                  "type": "mrkdwn",
                  "text": "*âš ï¸ LOW CONFIDENCE INCIDENT - HUMAN REVIEW NEEDED*\n\nThe AI is only {{ outputs.parse_ai_decision.vars.confidence_score }}% confident. Manual review required before taking action.\n\n*Incident Summary:*\n{{ outputs.parse_ai_decision.vars.incident_summary }}\n\n*Root Cause Hypothesis:*\n{{ outputs.parse_ai_decision.vars.root_cause_hypothesis }}\n\n*Detected Anomalies:*\n{{ outputs.anomaly_detection.vars.summary }}"
                }
              },
              {
                "type": "section",
                "fields": [
                  {
                    "type": "mrkdwn",
                    "text": "*Service:*\n{{ inputs.service_name }}"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*Deployment:*\n{{ outputs.parse_metrics.vars.deployment_id }}"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*Error Rate:*\n{{ outputs.parse_metrics.vars.error_rate }}%"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*P99 Latency:*\n{{ outputs.parse_metrics.vars.p99_latency }}ms"
                  }
                ]
              }
            ]
          }

  # PHASE 9 â€” ROUTE BY SEVERITY
  - id: route_by_severity
    type: io.kestra.plugin.core.flow.Switch
    value: "{{ outputs.parse_ai_decision.vars.severity }}"
    cases:
      P0:
        - id: handle_p0
          type: io.kestra.plugin.core.flow.Sequential
          tasks:
            - id: evaluate_rollback_decision
              type: io.kestra.plugin.core.flow.If
              condition: "{{ outputs.parse_ai_decision.vars.confidence_score >= inputs.min_confidence_for_rollback and outputs.parse_ai_decision.vars.recommended_action == 'rollback' }}"
              then:
                - id: check_github_token
                  type: io.kestra.plugin.core.flow.If
                  condition: "{{ inputs.github_token != 'mock' }}"
                  then:
                    # Real GitHub mode - trigger GitHub Actions workflow
                    - id: trigger_github_action
                      type: io.kestra.plugin.core.http.Request
                      uri: "https://api.github.com/repos/{{ inputs.deployments_repo }}/actions/workflows/rollback.yml/dispatches"
                      method: POST
                      headers:
                        Authorization: "token {{ inputs.github_token }}"
                        Accept: "application/vnd.github+json"
                        Content-Type: "application/json"
                        X-GitHub-Api-Version: "2022-11-28"
                      body: |
                        {
                          "ref": "main",
                          "inputs": {
                            "rollback_to": "{{ outputs.parse_ai_decision.vars.deployment_to_rollback }}",
                            "service": "{{ inputs.service_name }}",
                            "environment": "{{ inputs.environment }}",
                            "execution_id": "{{ execution.id }}",
                            "reason": "{{ outputs.parse_ai_decision.vars.incident_summary }}",
                            "confidence": "{{ outputs.parse_ai_decision.vars.confidence_score }}"
                          }
                        }
                      allowFailed: false

                    - id: log_github_action_triggered
                      type: io.kestra.plugin.core.log.Log
                      message: "âœ” GitHub Action workflow triggered for rollback â†’ deployment {{ outputs.parse_ai_decision.vars.deployment_to_rollback }}"

                    - id: wait_for_github_action
                      type: io.kestra.plugin.core.flow.Sleep
                      duration: PT15S

                    - id: log_github_action_status
                      type: io.kestra.plugin.core.log.Log
                      message: "â³ GitHub Action is executing. Check https://github.com/{{ inputs.deployments_repo }}/actions for progress"

                  else:
                    # Mock mode - direct API call to service
                    - id: trigger_mock_rollback
                      type: io.kestra.plugin.core.http.Request
                      uri: "{{ inputs.service_url }}/deployment/{{ outputs.parse_ai_decision.vars.deployment_to_rollback }}/activate"
                      method: POST
                      contentType: application/json
                      body: |
                        {
                          "reason": "AI-triggered rollback (mock mode) - {{ outputs.parse_ai_decision.vars.incident_summary }}",
                          "confidence": {{ outputs.parse_ai_decision.vars.confidence_score }},
                          "rollback_to": {{ outputs.parse_ai_decision.vars.deployment_to_rollback }},
                          "execution_id": "{{ execution.id }}"
                        }
                      allowFailed: true

                    - id: log_rollback_mock
                      type: io.kestra.plugin.core.log.Log
                      message: "ðŸ”„ [MOCK] Rollback triggered â†’ deployment {{ outputs.parse_ai_decision.vars.deployment_to_rollback }}"

                    - id: wait_for_mock_rollback
                      type: io.kestra.plugin.core.flow.Sleep
                      duration: PT5S

              else:
                - id: skip_rollback_log
                  type: io.kestra.plugin.core.log.Log
                  message: "â­ï¸ Rollback skipped - Confidence {{ outputs.parse_ai_decision.vars.confidence_score }}% or action '{{ outputs.parse_ai_decision.vars.recommended_action }}'"

            - id: trigger_pagerduty
              type: io.kestra.plugin.core.flow.If
              condition: "{{ inputs.pagerduty_token != 'mock' }}"
              then:
                - id: pagerduty_real_alert
                  type: io.kestra.plugin.core.http.Request
                  uri: https://events.pagerduty.com/v2/enqueue
                  method: POST
                  contentType: application/json
                  body: |
                    {
                      "routing_key": "{{ inputs.pagerduty_token }}",
                      "event_action": "trigger",
                      "payload": {
                        "summary": "P0 CRITICAL: {{ inputs.service_name }} ({{ inputs.environment }}) - {{ outputs.parse_ai_decision.vars.incident_summary }}",
                        "severity": "critical",
                        "source": "{{ inputs.service_name }}",
                        "component": "{{ inputs.environment }}",
                        "custom_details": {
                          "incident_summary": "{{ outputs.parse_ai_decision.vars.incident_summary }}",
                          "root_cause": "{{ outputs.parse_ai_decision.vars.root_cause_hypothesis }}",
                          "error_rate": "{{ outputs.parse_metrics.vars.error_rate }}%",
                          "p95_latency": "{{ outputs.parse_metrics.vars.p95_latency }}ms",
                          "p99_latency": "{{ outputs.parse_metrics.vars.p99_latency }}ms",
                          "cpu_usage": "{{ outputs.parse_metrics.vars.cpu_usage }}%",
                          "confidence": "{{ outputs.parse_ai_decision.vars.confidence_score }}%",
                          "anomalies": "{{ outputs.anomaly_detection.vars.summary }}",
                          "action_taken": "{{ outputs.parse_ai_decision.vars.recommended_action }}",
                          "deployment_id": "{{ outputs.parse_metrics.vars.deployment_id }}",
                          "execution_id": "{{ execution.id }}"
                        }
                      }
                    }
              else:
                - id: pagerduty_mock_alert
                  type: io.kestra.plugin.core.log.Log
                  message: "ðŸš¨ [MOCK] PagerDuty alert triggered - P0 CRITICAL: {{ inputs.service_name }} - {{ outputs.parse_ai_decision.vars.incident_summary }}"

            - id: slack_notify_p0
              type: io.kestra.plugin.core.http.Request
              uri: "{{ inputs.slack_webhook }}"
              method: POST
              contentType: application/json
              body: |
                {
                  "blocks": [
                    {
                      "type": "header",
                      "text": {
                        "type": "plain_text",
                        "text": "ðŸ”´ P0 CRITICAL INCIDENT",
                        "emoji": true
                      }
                    },
                    {
                      "type": "section",
                      "fields": [
                        {
                          "type": "mrkdwn",
                          "text": "*Service:*\n{{ inputs.service_name }}"
                        },
                        {
                          "type": "mrkdwn",
                          "text": "*Environment:*\n{{ inputs.environment }}"
                        },
                        {
                          "type": "mrkdwn",
                          "text": "*Severity:*\nP0"
                        },
                        {
                          "type": "mrkdwn",
                          "text": "*Confidence:*\n{{ outputs.parse_ai_decision.vars.confidence_score }}%"
                        }
                      ]
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Incident Summary:*\n{{ outputs.parse_ai_decision.vars.incident_summary }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Root Cause Hypothesis:*\n{{ outputs.parse_ai_decision.vars.root_cause_hypothesis }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Detected Anomalies:*\n{{ outputs.anomaly_detection.vars.summary }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Current Metrics:*\nâ€¢ Error Rate: {{ outputs.parse_metrics.vars.error_rate }}%\nâ€¢ P95 Latency: {{ outputs.parse_metrics.vars.p95_latency }}ms\nâ€¢ P99 Latency: {{ outputs.parse_metrics.vars.p99_latency }}ms\nâ€¢ CPU Usage: {{ outputs.parse_metrics.vars.cpu_usage }}%\nâ€¢ Deployment: {{ outputs.parse_metrics.vars.deployment_id }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Action Taken:*\n{{ outputs.parse_ai_decision.vars.recommended_action }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Deployment Source:*\n{{ outputs.consolidate_deployment_data.vars.source }}"
                      }
                    },
                    {
                      "type": "divider"
                    },
                    {
                      "type": "context",
                      "elements": [
                        {
                          "type": "mrkdwn",
                          "text": "ðŸš¨ PagerDuty alert triggered | Execution ID: {{ execution.id }} | Started: {{ execution.startDate }}"
                        }
                      ]
                    }
                  ]
                }

      P1:
        - id: handle_p1
          type: io.kestra.plugin.core.flow.Sequential
          tasks:
            - id: log_p1_incident
              type: io.kestra.plugin.core.log.Log
              message: "ðŸŸ  P1 MAJOR incident logged: {{ outputs.parse_ai_decision.vars.incident_summary }}"

            - id: trigger_pagerduty_p1
              type: io.kestra.plugin.core.flow.If
              condition: "{{ inputs.pagerduty_token != 'mock' }}"
              then:
                - id: pagerduty_p1_alert
                  type: io.kestra.plugin.core.http.Request
                  uri: https://events.pagerduty.com/v2/enqueue
                  method: POST
                  contentType: application/json
                  body: |
                    {
                      "routing_key": "{{ inputs.pagerduty_token }}",
                      "event_action": "trigger",
                      "payload": {
                        "summary": "P1 MAJOR: {{ inputs.service_name }} ({{ inputs.environment }}) - {{ outputs.parse_ai_decision.vars.incident_summary }}",
                        "severity": "error",
                        "source": "{{ inputs.service_name }}",
                        "component": "{{ inputs.environment }}",
                        "custom_details": {
                          "incident_summary": "{{ outputs.parse_ai_decision.vars.incident_summary }}",
                          "root_cause": "{{ outputs.parse_ai_decision.vars.root_cause_hypothesis }}",
                          "error_rate": "{{ outputs.parse_metrics.vars.error_rate }}%",
                          "p95_latency": "{{ outputs.parse_metrics.vars.p95_latency }}ms",
                          "p99_latency": "{{ outputs.parse_metrics.vars.p99_latency }}ms",
                          "confidence": "{{ outputs.parse_ai_decision.vars.confidence_score }}%",
                          "anomalies": "{{ outputs.anomaly_detection.vars.summary }}",
                          "deployment_id": "{{ outputs.parse_metrics.vars.deployment_id }}",
                          "execution_id": "{{ execution.id }}"
                        }
                      }
                    }
              else:
                - id: pagerduty_p1_mock
                  type: io.kestra.plugin.core.log.Log
                  message: "ðŸŸ  [MOCK] PagerDuty alert triggered - P1 MAJOR: {{ inputs.service_name }} - {{ outputs.parse_ai_decision.vars.incident_summary }}"

            - id: slack_notify_p1
              type: io.kestra.plugin.core.http.Request
              uri: "{{ inputs.slack_webhook }}"
              method: POST
              contentType: application/json
              body: |
                {
                  "blocks": [
                    {
                      "type": "header",
                      "text": {
                        "type": "plain_text",
                        "text": "ðŸŸ  P1 MAJOR INCIDENT",
                        "emoji": true
                      }
                    },
                    {
                      "type": "section",
                      "fields": [
                        {
                          "type": "mrkdwn",
                          "text": "*Service:*\n{{ inputs.service_name }}"
                        },
                        {
                          "type": "mrkdwn",
                          "text": "*Environment:*\n{{ inputs.environment }}"
                        },
                        {
                          "type": "mrkdwn",
                          "text": "*Severity:*\nP1"
                        },
                        {
                          "type": "mrkdwn",
                          "text": "*Confidence:*\n{{ outputs.parse_ai_decision.vars.confidence_score }}%"
                        }
                      ]
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Incident Summary:*\n{{ outputs.parse_ai_decision.vars.incident_summary }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Root Cause Hypothesis:*\n{{ outputs.parse_ai_decision.vars.root_cause_hypothesis }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Detected Anomalies:*\n{{ outputs.anomaly_detection.vars.summary }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Current Metrics:*\nâ€¢ Error Rate: {{ outputs.parse_metrics.vars.error_rate }}%\nâ€¢ P95 Latency: {{ outputs.parse_metrics.vars.p95_latency }}ms\nâ€¢ P99 Latency: {{ outputs.parse_metrics.vars.p99_latency }}ms\nâ€¢ CPU Usage: {{ outputs.parse_metrics.vars.cpu_usage }}%"
                      }
                    },
                    {
                      "type": "divider"
                    },
                    {
                      "type": "context",
                      "elements": [
                        {
                          "type": "mrkdwn",
                          "text": "ðŸš¨ PagerDuty alert triggered | No automatic rollback | Execution ID: {{ execution.id }}"
                        }
                      ]
                    }
                  ]
                }

      P2:
        - id: handle_p2
          type: io.kestra.plugin.core.flow.Sequential
          tasks:
            - id: log_p2_incident
              type: io.kestra.plugin.core.log.Log
              message: "ðŸŸ¡ P2 incident logged: {{ outputs.parse_ai_decision.vars.incident_summary }}"

            - id: notify_p2_slack
              type: io.kestra.plugin.core.http.Request
              uri: "{{ inputs.slack_webhook }}"
              method: POST
              contentType: application/json
              body: |
                {
                  "blocks": [
                    {
                      "type": "header",
                      "text": {
                        "type": "plain_text",
                        "text": "ðŸŸ¡ P2 Incident",
                        "emoji": true
                      }
                    },
                    {
                      "type": "section",
                      "fields": [
                        {
                          "type": "mrkdwn",
                          "text": "*Service:*\n{{ inputs.service_name }}"
                        },
                        {
                          "type": "mrkdwn",
                          "text": "*Confidence:*\n{{ outputs.parse_ai_decision.vars.confidence_score }}%"
                        }
                      ]
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Summary:*\n{{ outputs.parse_ai_decision.vars.incident_summary }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "*Metrics:*\nError Rate: {{ outputs.parse_metrics.vars.error_rate }}% | P99: {{ outputs.parse_metrics.vars.p99_latency }}ms"
                      }
                    },
                    {
                      "type": "context",
                      "elements": [
                        {
                          "type": "mrkdwn",
                          "text": "ðŸ“Š Monitored during business hours â€¢ No immediate action required"
                        }
                      ]
                    }
                  ]
                }

      P3:
        - id: handle_p3
          type: io.kestra.plugin.core.log.Log
          message: "P3 normal state - minor issue logged: {{ outputs.parse_ai_decision.vars.incident_summary }}"

      default:
        - id: handle_invalid_ai_output
          type: io.kestra.plugin.core.log.Log
          message: "Invalid AI output or missing severity."

  # PHASE 10 â€” HEALTH CHECK AFTER REMEDIATION (Skip for P3)
  - id: post_remediation_health_check
    type: io.kestra.plugin.core.flow.If
    condition: "{{ outputs.parse_ai_decision.vars.severity != 'P3' }}"
    then:
      - id: wait_for_stabilization
        type: io.kestra.plugin.core.flow.Sleep
        duration: PT10S

      - id: fetch_post_metrics
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.metrics_url }}"
        method: GET

      - id: fetch_post_alerts
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.alerts_url }}"
        method: GET

      - id: parse_post_metrics
        type: io.kestra.plugin.scripts.python.Script
        docker:
          image: python:3.11-slim
        beforeCommands:
          - pip install kestra
        script: |
          import json
          import sys
          from kestra import Kestra

          try:
              raw_body = """{{ outputs.fetch_post_metrics.body }}"""
              
              if isinstance(raw_body, dict):
                  metrics = raw_body
              elif isinstance(raw_body, str):
                  metrics = json.loads(raw_body)
              else:
                  raise ValueError(f"Unexpected type: {type(raw_body)}")
              
              m = metrics.get('metrics', {})
              
              Kestra.outputs({
                  'error_rate': float(m.get('error_rate_percent', 0)),
                  'p99_latency': float(m.get('p99_latency_ms', 0)),
                  'deployment_id': int(metrics.get('deployment_id', 0)),
                  'health_status': str(metrics.get('health_status', 'unknown'))
              })
              
              print(f" Post-metrics parsed: deployment={metrics.get('deployment_id')}, error={m.get('error_rate_percent')}%", file=sys.stderr)
              
          except Exception as e:
              print(f" Error parsing post-metrics: {e}", file=sys.stderr)
              Kestra.outputs({
                  'error_rate': 0,
                  'p99_latency': 0,
                  'deployment_id': 0,
                  'health_status': 'unknown'
              })

      - id: compare_health_status
        type: io.kestra.plugin.scripts.python.Script
        docker:
          image: python:3.11-slim
        beforeCommands:
          - pip install kestra
        script: |
          import json
          import sys
          from kestra import Kestra

          before_error_rate = {{ outputs.parse_metrics.vars.error_rate }}
          after_error_rate = {{ outputs.parse_post_metrics.vars.error_rate }}

          before_p99 = {{ outputs.parse_metrics.vars.p99_latency }}
          after_p99 = {{ outputs.parse_post_metrics.vars.p99_latency }}

          before_deployment = {{ outputs.parse_metrics.vars.deployment_id }}
          after_deployment = {{ outputs.parse_post_metrics.vars.deployment_id }}

          if before_error_rate > 0:
              error_improvement_pct = ((before_error_rate - after_error_rate) / before_error_rate) * 100
          else:
              error_improvement_pct = 0

          if before_p99 > 0:
              latency_improvement_pct = ((before_p99 - after_p99) / before_p99) * 100
          else:
              latency_improvement_pct = 0

          if after_error_rate < 1 and after_p99 < 200:
              health_status = "FULLY_RECOVERED"
          elif after_error_rate < before_error_rate * 0.5 and latency_improvement_pct > 30:
              health_status = "SIGNIFICANT_IMPROVEMENT"
          elif after_error_rate < before_error_rate or after_p99 < before_p99:
              health_status = "IMPROVING"
          elif after_error_rate == before_error_rate and after_p99 == before_p99:
              health_status = "STABLE"
          else:
              health_status = "DEGRADED"

          Kestra.outputs({
              'health_status': health_status,
              'before_error_rate': before_error_rate,
              'after_error_rate': after_error_rate,
              'error_improvement_pct': round(error_improvement_pct, 2),
              'before_p99_latency': before_p99,
              'after_p99_latency': after_p99,
              'latency_improvement_pct': round(latency_improvement_pct, 2),
              'deployment_changed': before_deployment != after_deployment
          })

          print(f"Health check complete: {health_status}", file=sys.stderr)
          print(f"Error rate: {before_error_rate}% -> {after_error_rate}% ({error_improvement_pct:.2f}% improvement)", file=sys.stderr)
          print(f"P99 latency: {before_p99}ms -> {after_p99}ms ({latency_improvement_pct:.2f}% improvement)", file=sys.stderr)
          print(f"Deployment: {before_deployment} -> {after_deployment}", file=sys.stderr)

      - id: notify_health_status
        type: io.kestra.plugin.core.http.Request
        uri: "{{ inputs.slack_webhook }}"
        method: POST
        contentType: application/json
        body: |
          {
            "blocks": [
              {
                "type": "header",
                "text": {
                  "type": "plain_text",
                  "text": "ðŸ“Š Post-Remediation Health Check",
                  "emoji": true
                }
              },
              {
                "type": "section",
                "fields": [
                  {
                    "type": "mrkdwn",
                    "text": "*Status:*\n{{ outputs.compare_health_status.vars.health_status }}"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*Error Rate:*\n{{ outputs.compare_health_status.vars.before_error_rate }}% â†’ {{ outputs.compare_health_status.vars.after_error_rate }}%"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*P99 Latency:*\n{{ outputs.compare_health_status.vars.before_p99_latency }}ms â†’ {{ outputs.compare_health_status.vars.after_p99_latency }}ms"
                  },
                  {
                    "type": "mrkdwn",
                    "text": "*Improvement:*\nError: {{ outputs.compare_health_status.vars.error_improvement_pct }}%\nLatency: {{ outputs.compare_health_status.vars.latency_improvement_pct }}%"
                  }
                ]
              },
              {
                "type": "context",
                "elements": [
                  {
                    "type": "mrkdwn",
                    "text": "Incident: {{ outputs.parse_ai_decision.vars.severity }} | Execution: {{ execution.id }}"
                  }
                ]
              }
            ]
          }

  # PHASE 11 â€” SAVE INCIDENT SNAPSHOT
  - id: save_incident_snapshot
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    beforeCommands:
      - pip install kestra
    script: |
      import json
      from kestra import Kestra

      # Convert booleans safely
      chaos_mode = "{{ inputs.chaos_mode }}".lower() == "true"
      pagerduty_used = "{{ inputs.pagerduty_token }}" != "mock"
      severity = "{{ outputs.parse_ai_decision.vars.severity }}"

      # Check if health check ran (only for P0/P1/P2)
      health_check_ran = severity != "P3"

      snapshot = {
          "execution_id": "{{ execution.id }}",
          "timestamp": "{{ execution.startDate }}",
          "service": "{{ inputs.service_name }}",
          "environment": "{{ inputs.environment }}",
          "chaos_mode": chaos_mode,
          "severity": severity,
          "confidence": float("{{ outputs.parse_ai_decision.vars.confidence_score }}"),
          "summary": "{{ outputs.parse_ai_decision.vars.incident_summary }}",
          "root_cause": "{{ outputs.parse_ai_decision.vars.root_cause_hypothesis }}",
          "action_taken": "{{ outputs.parse_ai_decision.vars.recommended_action }}",
          "deployment_source": "{{ outputs.consolidate_deployment_data.vars.source }}",
          "pagerduty_priority": "{{ outputs.parse_ai_decision.vars.pagerduty_priority }}",

          "anomalies": {
              "count": int("{{ outputs.anomaly_detection.vars.count }}"),
              "summary": "{{ outputs.anomaly_detection.vars.summary }}"
          },

          "metrics": {
              "before": {
                  "error_rate": float("{{ outputs.parse_metrics.vars.error_rate }}"),
                  "p99_latency": float("{{ outputs.parse_metrics.vars.p99_latency }}"),
                  "p95_latency": float("{{ outputs.parse_metrics.vars.p95_latency }}"),
                  "cpu_usage": float("{{ outputs.parse_metrics.vars.cpu_usage }}"),
                  "memory_usage": float("{{ outputs.parse_metrics.vars.memory_usage }}"),
                  "deployment_id": int("{{ outputs.parse_metrics.vars.deployment_id }}"),
                  "health_status": "{{ outputs.parse_metrics.vars.health_status }}"
              }
          },

          "integrations": {
              "github": "{{ outputs.consolidate_deployment_data.vars.source }}",
              "pagerduty": pagerduty_used,
              "slack": True
          }
      }

      # Only include health check data if it ran (P0/P1/P2)
      if health_check_ran:
          deployment_changed = "{{ outputs.compare_health_status.vars.deployment_changed | default('false') }}".lower() == "true"
          
          snapshot["metrics"]["after"] = {
              "error_rate": float("{{ outputs.compare_health_status.vars.after_error_rate | default('0') }}"),
              "p99_latency": float("{{ outputs.compare_health_status.vars.after_p99_latency | default('0') }}"),
              "deployment_id": int("{{ outputs.parse_post_metrics.vars.deployment_id | default('0') }}"),
              "health_status": "{{ outputs.parse_post_metrics.vars.health_status | default('unknown') }}"
          }
          
          snapshot["metrics"]["improvement"] = {
              "error_rate_pct": float("{{ outputs.compare_health_status.vars.error_improvement_pct | default('0') }}"),
              "latency_pct": float("{{ outputs.compare_health_status.vars.latency_improvement_pct | default('0') }}")
          }
          
          snapshot["health_status"] = "{{ outputs.compare_health_status.vars.health_status | default('unknown') }}"
          snapshot["deployment_changed"] = deployment_changed
      else:
          # P3 - no health check data
          snapshot["health_status"] = "not_checked"
          snapshot["deployment_changed"] = False

      Kestra.outputs(snapshot)

      print("=" * 60)
      print("INCIDENT SNAPSHOT SAVED")
      print(json.dumps(snapshot, indent=2))
      print("=" * 60)

triggers:
  - id: scheduled_monitor
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "*/5 * * * *"
